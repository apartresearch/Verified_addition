{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3OoV2Pd-w7g"
      },
      "source": [
        "# Accurate Integer Addition in Transformers - Train the Model\n",
        "\n",
        "This CoLab defines, trains and analyses a Transformer model that performs integer addition e.g. 33357+82243=115600. Each digit is a separate token. For 5 digit addition, the model is given 12 \"question\" (input) tokens, and must then predict the corresponding 6 \"answer\" (output) tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVn4muC-5p1"
      },
      "source": [
        "For 5-digit 2-layer case with 30K training epochs this model has very low loss (2.3e-8) and seems to be 100% accurate. This CoLab trains the model, storing the results to Google Drive.\n",
        "\n",
        "(This CoLab follows on from the [Understanding Addition in Transformers](https://github.com/apartresearch/conceptual-interp/blob/main/Understanding_Addition_in_Transformers.ipynb)\n",
        "which explains a 1-layer integer addition model and documents a rare high-loss use case called \"Use Sum 9 Cascade\".)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWXJvUUb-6in"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rl41loM_Apt"
      },
      "source": [
        "# Part 1: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8Z1TVBObuOKF"
      },
      "outputs": [],
      "source": [
        "# Tokens used in vocab. (Token indexes 0 to 9 represent digits 0 to 9)\n",
        "PLUS_INDEX = 10\n",
        "MINUS_INDEX = 11\n",
        "EQUALS_INDEX = 12\n",
        "MAX_INDEX = EQUALS_INDEX\n",
        "\n",
        "class Config():\n",
        "  #@markdown Model\n",
        "  n_layers: int = 2 #@param\n",
        "  n_heads: int = 3 #@param\n",
        "\n",
        "  d_vocab: int = MAX_INDEX+1\n",
        "  d_model: int = ( 512 // n_heads ) * n_heads # About 512, and divisible by n_heads\n",
        "  d_mlp: int = 4 * d_model\n",
        "  d_head: int = d_model // n_heads  # About 170 when n_heads == 3\n",
        "  seed: int = 129000 #@param\n",
        "\n",
        "  #@markdown Data\n",
        "  n_digits: int = 5 #@param\n",
        "  n_ctx: int = 3 * n_digits + 3\n",
        "  act_fn: str = 'relu'\n",
        "  batch_size: int = 64 #@param\n",
        "\n",
        "  #@markdown Optimizer\n",
        "  n_training_steps: int = 30000 #@param\n",
        "  lr: float = 0.00008 #@param\n",
        "  weight_decay: int = 0.1 #@param\n",
        "\n",
        "  # Save graphs to CoLab temp files as PDF and HTML. Can manually export files for re-use in papers.\n",
        "  save_graph_to_file: bool = True\n",
        "\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def file_name_suffix(digits, layers, heads, d_model, d_head, ctx, seed, training_steps):\n",
        "  epoch_str = str(training_steps//1000) + \"K\"\n",
        "  return '_d{}_l{}_h{}_dm{}_dh{}_ctx{}_seed{}_train{}'.format(digits, layers, heads, d_model, d_head, ctx, seed, epoch_str)\n",
        "\n",
        "main_fname_suffix = 'add' + file_name_suffix(cfg.n_digits, cfg.n_layers, cfg.n_heads, cfg.d_model, cfg.d_head, cfg.n_ctx, cfg.seed, cfg.n_training_steps)\n",
        "\n",
        "\n",
        "def print_config():\n",
        "  print(main_fname_suffix)\n",
        "\n",
        "print_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weGMHx9k3OYh",
        "outputId": "3705e2e5-d6f7-4b4e-a513-84b973928703"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add_d5_l2_h3_dm510_dh170_ctx18_seed129000_train30K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5K2kA0L_FHc"
      },
      "source": [
        "# Part 2: Import libraries\n",
        "Imports standard libraries. Will ask for access to your Google to write model weightings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iS7C7n1bubkW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViLbdGWRudlt"
      },
      "outputs": [],
      "source": [
        "# Training saves the trained model weights to a file in your Google Drive. You will need to give permission for this CoLab to access your Google Drive.\n",
        "# Loading loads the model from your Google Drive. Avoids the say 10mins spent on training the model.\n",
        "\n",
        "GLOBAL=True\n",
        "if GLOBAL:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    rootdir=Path('/content/drive/MyDrive/AI/CoLabOutput/')\n",
        "else:\n",
        "    rootdir=Path('./')\n",
        "\n",
        "main_fname_full = main_fname_suffix + '.pth'\n",
        "\n",
        "model_save_location = rootdir/f'{main_fname_full}'\n",
        "\n",
        "print('model will save to {}'.format(str(model_save_location)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBOAqd0ZuhAV"
      },
      "outputs": [],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    %pip install kaleido\n",
        "    %pip install transformer_lens\n",
        "    #%pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "    %pip install circuitsvis\n",
        "    %pip install torchtyping\n",
        "    %pip install transformers\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "295MWK2owFNa"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h1MGZnAwHZz"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJpHPxqhwMeQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import circuitsvis as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS-PO-BPwo8f"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tExv4rk_L0C"
      },
      "source": [
        "# Part 3: Create model\n",
        "This section defines the token embedding / unembedding and creates the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqGmiutQwp22"
      },
      "outputs": [],
      "source": [
        "# Embedding / Unembedding\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    tokens = utils.to_numpy(tokens)\n",
        "    x = \"\".join([str(i) for i in tokens[:cfg.n_digits]])\n",
        "    y = \"\".join([str(i) for i in tokens[cfg.n_digits+1:cfg.n_digits*2+1]])\n",
        "    z = \"\".join([str(i) for i in tokens[cfg.n_ctx-cfg.n_digits-1:]])\n",
        "    equals = \"=\"\n",
        "    operator = \"+\"\n",
        "    return f\"{x}{operator}{y}{equals}{z}\"\n",
        "\n",
        "def string_to_tokens(string, batch: bool=False):\n",
        "    lookup = {str(i):i for i in range(10)}\n",
        "    lookup['+']=PLUS_INDEX\n",
        "    lookup['-']=MINUS_INDEX\n",
        "    lookup['=']=EQUALS_INDEX\n",
        "\n",
        "    tokens = [lookup[i] for i in string if i not in '\\n ']\n",
        "    if batch:\n",
        "        return torch.tensor(tokens)[None, :]\n",
        "    else:\n",
        "        return torch.tensor(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg6uwYzew4u5"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = HookedTransformerConfig(\n",
        "    n_layers = cfg.n_layers,\n",
        "    n_heads = cfg.n_heads,\n",
        "    d_model = cfg.d_model,\n",
        "    d_head = cfg.d_head,\n",
        "    d_mlp = cfg.d_mlp,\n",
        "    act_fn = cfg.act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab = cfg.d_vocab,\n",
        "    d_vocab_out = cfg.d_vocab,\n",
        "    n_ctx = cfg.n_ctx,\n",
        "    init_weights = True,\n",
        "    device = \"cuda\",\n",
        "    seed = cfg.seed,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(ht_cfg)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(),\n",
        "                        lr = cfg.lr,\n",
        "                        weight_decay = cfg.weight_decay,\n",
        "                        betas = (0.9, 0.98))\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg267eav_QSl"
      },
      "source": [
        "# Part 4: Data Generator. Addition sub-task categorisation\n",
        "This section defines the loss function and the training/tesing data generator.\n",
        "\n",
        "It also defines functions to categorise the training data by the addition sub-task defined in the paper. The addition sub tasks are abbreviated as:\n",
        "- BA is Base Add. Calculates the sum of two digits Dn and Dn' modulo 10, ignoring any carry over from previous columns.\n",
        "- MC1 is Make Carry 1. Evaluates to true if adding digits Dn and Dn' results in a carry over of 1 to the next column.\n",
        "- MS9 is Make Sum 9. Evaluates to true if adding digits Dn and Dn' gives exactly 9.\n",
        "- UC1 is Use Carry 1. Takes the previous column's carry output and adds it to the sum of the current digit pair.\n",
        "- US9 is Use Sum 9. Propagates (aka cascades) a carry over of 1 to the next column if the current column sums to 9 and the previous column generated a carry over. US9 is the most complex task as it spans three digits. For some rare questions (e.g. 00555 + 00445 = 01000) US9 applies to up to four sequential digits, causing a chain effect, with the MC1 cascading through multiple digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIVxDpq0w7WY"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "\n",
        "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
        "def logits_to_tokens_loss(logits, tokens):\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the addition answer probabilities\n",
        "  ans_logits = logits[:, -(cfg.n_digits+2):-1]\n",
        "\n",
        "  # Convert raw score (logits) vector into a probability distribution.\n",
        "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
        "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
        "\n",
        "  max_indices = torch.argmax(ans_probs, dim=-1)\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the model’s answer.\n",
        "  ans_tokens = tokens[:, -(cfg.n_digits+1):]\n",
        "\n",
        "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
        "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
        "\n",
        "  return ans_loss, max_indices\n",
        "\n",
        "# Calculate loss as negative of average per-token mean probability\n",
        "def loss_fn(ans_loss):\n",
        "  return -ans_loss.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dps1g8spw-U4"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" data generator function. Invoked using next().\n",
        "# \"Addition\" batch entries are formated XXXXX+YYYYY=ZZZZZZ e.g. 55003+80002=135005\n",
        "# \"Subtraction\" batch entries are formated XXXXX-YYYYY=ZZZZZZ e.g. 55003-80002=-24999, 80002-55003=024999\n",
        "# Note that answer has one more digit than the question\n",
        "# Returns characteristics of each batch entry to aid later analysis\n",
        "def data_generator():\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    while True:\n",
        "        #generate a batch of questions (answers calculated below)\n",
        "        batch = torch.zeros((cfg.batch_size, cfg.n_ctx)).to(torch.int64)\n",
        "        x = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "        y = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "\n",
        "\n",
        "        # The UseSum9 task is compound and rare and so hard to learn.\n",
        "        # For some batches, we increase the MakeSum9 case frequency\n",
        "        # UseSum9 also relies on MakeCarry1 (50%) from previous column.\n",
        "        # So UseSum9 frequency is increased by 60% * 40% * 50% = 12%\n",
        "        if random.randint(1, 5) < 3: # 60%\n",
        "          # Flatten x and y to 1D tensors\n",
        "          x_flat = x.view(-1)\n",
        "          y_flat = y.view(-1)\n",
        "\n",
        "          num_elements_to_modify = int(0.40 * x.numel()) # 40%\n",
        "          indices_to_modify = torch.randperm(x_flat.numel())[:num_elements_to_modify]\n",
        "          if random.randint(1, 2) == 1:\n",
        "            x_flat[indices_to_modify] = 9 - y_flat[indices_to_modify]\n",
        "          else:\n",
        "            y_flat[indices_to_modify] = 9 - x_flat[indices_to_modify]\n",
        "\n",
        "          # Reshape x and y back to its original shape\n",
        "          x = x_flat.view(x.shape)\n",
        "          y = y_flat.view(x.shape)\n",
        "\n",
        "\n",
        "        batch[:, :cfg.n_digits] = x\n",
        "        batch[:, cfg.n_digits] = PLUS_INDEX\n",
        "        batch[:, 1+cfg.n_digits:1+cfg.n_digits*2] = y\n",
        "        batch[:, 1+cfg.n_digits*2] = EQUALS_INDEX\n",
        "\n",
        "        # These attributes are used for testing addition\n",
        "        base_adds = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        make_carry1s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        sum9s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        use_carry1s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        use_sum9s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "\n",
        "        # generate the addition question answers & other info for testing\n",
        "        for i in range(cfg.n_digits):\n",
        "            # the column in the test attributes being updated\n",
        "            test_col = cfg.n_digits-1-i\n",
        "\n",
        "            base_add = batch[:, cfg.n_digits-1-i] + batch[:, 2*cfg.n_digits-i]\n",
        "            base_adds[:, test_col] = base_add % 10\n",
        "\n",
        "            sum9 = (base_add == 9)\n",
        "            sum9s[:, test_col] = sum9\n",
        "\n",
        "            if i>0:\n",
        "              use_carry1s[:, test_col] = make_carry1s[:, test_col+1]\n",
        "            use_carry = use_carry1s[:, test_col]\n",
        "\n",
        "            use_sum9s[:, test_col] = sum9 & use_carry;\n",
        "\n",
        "            digit_sum = base_add + use_carry1s[:, test_col]\n",
        "\n",
        "            make_carry = (digit_sum >= 10)\n",
        "            make_carry1s[:, test_col] = make_carry\n",
        "\n",
        "            batch[:, -1-i] = (digit_sum % 10)\n",
        "\n",
        "        # Final (possible) carry to highest digit of the sum\n",
        "        batch[:, -1-cfg.n_digits] = make_carry1s[:, 0]\n",
        "\n",
        "        yield batch.cuda(), base_adds.cuda(), make_carry1s.cuda(), sum9s.cuda(), use_carry1s.cuda(), use_sum9s.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH_rXfA2xAIG"
      },
      "outputs": [],
      "source": [
        "ds = data_generator()\n",
        "\n",
        "tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "\n",
        "print(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JDY_Fqa_Wfb"
      },
      "source": [
        "# Part 5: Train model with Infinite Data\n",
        "Train model for n_training_steps, storing train_losses per epoch.\n",
        "\n",
        "Each training step (of n_training_steps) new training data (a batch of batch_size tokens) is generated and the model is trained and loss calculated on it. No separate \"testing\" data is needed, as the training data is unique each step. Memorisation of past training data by the model (if any) is minimally beneficial. For 5 digit addition there are 10 billion possible questions, and model training is on ~2 million questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ61_nfKxI9c"
      },
      "outputs": [],
      "source": [
        "# Initialise the data generator\n",
        "ds = data_generator()\n",
        "\n",
        "# Train the model\n",
        "train_losses_list = []\n",
        "per_token_train_losses_list = []\n",
        "\n",
        "for epoch in tqdm.tqdm(range(cfg.n_training_steps)):\n",
        "\n",
        "  tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "  logits = model(tokens)\n",
        "\n",
        "  per_token_train_losses_raw, _ = logits_to_tokens_loss(logits, tokens)\n",
        "  per_token_train_losses = loss_fn(per_token_train_losses_raw)\n",
        "  per_token_train_losses_list.append(utils.to_numpy(per_token_train_losses))\n",
        "\n",
        "  train_loss = per_token_train_losses.mean()\n",
        "  train_loss.backward()\n",
        "  train_losses_list.append(train_loss.item())\n",
        "\n",
        "  optimizer.step()\n",
        "  scheduler.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    the_loss = train_loss.item()\n",
        "    print(epoch, the_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgzjamSn2y4j"
      },
      "outputs": [],
      "source": [
        "print(\"Saving model to file\", model_save_location)\n",
        "torch.save(model.state_dict(), model_save_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc4pQgE__cDp"
      },
      "source": [
        "# Part 6: Final Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwJBrnSJ23eb"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "\n",
        "final_training_loss = round((train_losses_list[-5]+train_losses_list[-4]+train_losses_list[-3]+train_losses_list[-2]+train_losses_list[-1])/5,9)\n",
        "print( \"Final training loss\", final_training_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiOdvQFD_fxZ"
      },
      "source": [
        "# Part 7: Line Graphs\n",
        "\n",
        "This section analyses the training loss by graphing it at a high level.\n",
        "\n",
        "The loss curve for all digits show visible inflection points (bumps), but is too high level to help understand the algorithm.\n",
        "\n",
        "When this graph is decomposed into 'per digit' graphs, the interesting distinct 'per digit' curves appear, showing each digit is being refined semi-independently, with the model algorithm refining each digit separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmyT2TFd29at"
      },
      "outputs": [],
      "source": [
        "epochs_to_graph=1200\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "# Helper function to plot multiple lines\n",
        "def lines(raw_lines_list, x=None, mode='lines', labels=None, xaxis='Epoch', yaxis='Loss', title = '', log_y=False, hover=None, all_epochs=True, **kwargs):\n",
        "\n",
        "    lines_list = raw_lines_list if all_epochs==False else [row[:epochs_to_graph] for row in raw_lines_list]\n",
        "    log_suffix = '' if log_y==False else ' (Log)'\n",
        "    epoch_suffix = '' if all_epochs==False else ' (' + str(epochs_to_graph) + ' steps)'\n",
        "    full_title = title + log_suffix + epoch_suffix\n",
        "\n",
        "    if type(lines_list)==torch.Tensor:\n",
        "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
        "    if x is None:\n",
        "        x=np.arange(len(lines_list[0]))\n",
        "    if cfg.save_graph_to_file :\n",
        "      fig = go.Figure(layout={})\n",
        "      print(full_title)\n",
        "    else:\n",
        "      fig = go.Figure(layout={'title':full_title})\n",
        "\n",
        "    fig.update_xaxes(title=xaxis)\n",
        "    fig.update_yaxes(title=yaxis + log_suffix)\n",
        "    for c, line in enumerate(lines_list):\n",
        "        if type(line)==torch.Tensor:\n",
        "            line = utils.to_numpy(line)\n",
        "        if labels is not None:\n",
        "            label = labels[c]\n",
        "        else:\n",
        "            label = c\n",
        "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
        "    if log_y:\n",
        "        fig.update_layout(yaxis_type=\"log\")\n",
        "    if cfg.save_graph_to_file:\n",
        "        fig.update_layout(margin=dict(l=10, r=10, t=10, b=10),width=1200,height=300)\n",
        "\n",
        "    fig.show(bbox_inches=\"tight\")\n",
        "\n",
        "    if cfg.save_graph_to_file:\n",
        "        filename = full_title.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"&\", \"\").replace(\",\", \"\").replace(\"%\", \"\")   +'.pdf'\n",
        "        pio.write_image(fig, filename)\n",
        "\n",
        "\n",
        "\n",
        "title_suffix = ' Loss Curves for ' + str(cfg.n_digits) + ' digit addition'\n",
        "per_token_losses = np.stack(per_token_train_losses_list, axis=0)\n",
        "\n",
        "line(train_losses_list,\n",
        "    title=title_suffix)\n",
        "\n",
        "all_epochs = True;\n",
        "for i in range(2):\n",
        "  lines([per_token_losses[:, i] for i in range(1+cfg.n_digits)]+[train_losses_list],\n",
        "        labels = [f'digit {i}' for i in range(1+cfg.n_digits)]+['all_digits'],\n",
        "        title='Per digit'+title_suffix,\n",
        "        all_epochs=all_epochs)\n",
        "\n",
        "  all_epochs = False;\n",
        "\n",
        "for i in range(1+cfg.n_digits):\n",
        "  print('Final Loss for digit ' + str(i) + ' is ', per_token_losses[-1, i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgk8m02A_lxB"
      },
      "source": [
        "# Part 8: Questions Set Up\n",
        "\n",
        "Create sets of sample questions (by task) to ask the model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjM6eWDm3Ap7"
      },
      "outputs": [],
      "source": [
        "# Insert a number into the question\n",
        "def insert_question_number(the_question, index, first_digit_index, the_digits, n):\n",
        "\n",
        "  last_digit_index = first_digit_index + the_digits - 1\n",
        "\n",
        "  for j in range(the_digits):\n",
        "    the_question[index, last_digit_index-j] = n % 10\n",
        "    n = n // 10\n",
        "\n",
        "\n",
        "# Create a single question\n",
        "def make_a_question(the_question, index, q1, q2):\n",
        "  a = q1 + q2\n",
        "\n",
        "  insert_question_number(the_question, index, 0, cfg.n_digits, q1)\n",
        "\n",
        "  the_question[index, cfg.n_digits] = PLUS_INDEX\n",
        "\n",
        "  insert_question_number( the_question, index, cfg.n_digits+1, cfg.n_digits, q2)\n",
        "\n",
        "  the_question[index, 2*cfg.n_digits+1] = EQUALS_INDEX\n",
        "  offset = 2\n",
        "\n",
        "  insert_question_number(the_question, index, 2*cfg.n_digits + offset, cfg.n_digits+1, q1+q2)\n",
        "\n",
        "\n",
        "# Create a batch of questions from a 2D matrix of ints\n",
        "def make_questions(q_matrix):\n",
        "  length = len(q_matrix)\n",
        "\n",
        "  questions = torch.zeros((length, cfg.n_ctx)).to(torch.int64)\n",
        "\n",
        "  limit = 10 ** cfg.n_digits\n",
        "  for i in range(length):\n",
        "    if (q_matrix[i][0] < limit) and (q_matrix[i][1] < limit) :\n",
        "      make_a_question(questions, i, q_matrix[i][0], q_matrix[i][1])\n",
        "\n",
        "  return questions\n",
        "\n",
        "\n",
        "def prediction_to_string(max_indices):\n",
        "  answer = \"\".join([str(i) for i in utils.to_numpy(max_indices)[0]])\n",
        "  return answer;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7tHBz7V3DL_"
      },
      "outputs": [],
      "source": [
        "# Analyse the question and return the use case as BA, MC, SimpleUS9 or CascadeUS9\n",
        "def get_question_case(q):\n",
        "  qa = utils.to_numpy(q)\n",
        "  qn = qa[:2*cfg.n_digits+2]\n",
        "\n",
        "  # Locate the MC and MS digits (if any)\n",
        "  mc = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  ms = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if qn[dn] + qn[dn + cfg.n_digits + 1] == 9:\n",
        "      ms[cfg.n_digits-1-dn] = 1\n",
        "    if qn[dn] + qn[dn + cfg.n_digits +1] > 9:\n",
        "      mc[cfg.n_digits-1-dn] = 1\n",
        "\n",
        "  # Calculate the use case of a question\n",
        "  if torch.sum(mc) == 0:\n",
        "    return \"BA\"\n",
        "\n",
        "  if torch.sum(ms) == 0:\n",
        "    return \"MC1\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-2 and mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1:\n",
        "      return \"CascadeUS9\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-1 and mc[dn] == 1 and ms[dn+1] == 1:\n",
        "      return \"SimpleUS9\"\n",
        "\n",
        "  return \"MC1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BpOcko53GrI"
      },
      "outputs": [],
      "source": [
        "# Manually create some questions that strongly test one use case\n",
        "\n",
        "\n",
        "def make_ba_questions():\n",
        "    return make_questions(\n",
        "      [[12345, 33333],\n",
        "      [33333, 12345],\n",
        "      [45762, 33113],\n",
        "      [888, 11111],\n",
        "      [2362, 23123],\n",
        "      [15, 81],\n",
        "      [1000, 4440],\n",
        "      [4440, 1000],\n",
        "      [24033, 25133],\n",
        "      [23533, 21133],\n",
        "      [32500, 1],\n",
        "      [31500, 1111],\n",
        "      [5500, 12323],\n",
        "      [4500, 2209],\n",
        "      [ 33345, 66643], # =099988\n",
        "      [ 66643, 33345], # =099988\n",
        "      [10990, 44000],\n",
        "      [60000, 30000],\n",
        "      [10000, 20000]])\n",
        "\n",
        "\n",
        "def make_uc1_questions():\n",
        "    return make_questions(\n",
        "      [[ 15, 45],\n",
        "      [ 25, 55],\n",
        "      [ 35, 59],\n",
        "      [ 40035, 40049],\n",
        "      [ 5025, 5059],\n",
        "      [ 15, 65],\n",
        "      [ 44000, 46000],\n",
        "      [ 70000, 40000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 1500, 4500],\n",
        "      [ 2500, 5500],\n",
        "      [ 3500, 5900],\n",
        "      [ 15020, 45091],\n",
        "      [ 25002, 55019],\n",
        "      [ 35002, 59019]])\n",
        "\n",
        "\n",
        "def make_simple_us9_questions():\n",
        "    return make_questions(\n",
        "      [[ 55, 45],\n",
        "      [ 45, 55],\n",
        "      [ 45, 59],\n",
        "      [ 35, 69],\n",
        "      [ 25, 79],\n",
        "      [ 15, 85],\n",
        "      [ 15, 88],\n",
        "      [ 15508, 14500],\n",
        "      [ 14508, 15500],\n",
        "      [ 24533, 25933],\n",
        "      [ 23533, 26933],\n",
        "      [ 32500, 7900],\n",
        "      [ 31500, 8500],\n",
        "      [ 550, 450],\n",
        "      [ 450, 550],\n",
        "      [ 10880, 41127],\n",
        "      [ 41127, 10880],\n",
        "      [ 12386, 82623]])\n",
        "\n",
        "\n",
        "def make_cascade_us9_questions(clean = True):\n",
        "    return make_questions(\n",
        "      # These are two level UseSum9 cascades\n",
        "      [[ 555, 445],\n",
        "      [ 3340, 6660],\n",
        "      [ 8880, 1120],\n",
        "      [ 1120, 8880],\n",
        "      [ 123, 877],\n",
        "      [ 877, 123],\n",
        "      [ 321, 679],\n",
        "      [ 679, 321],\n",
        "      [ 1283, 88786],\n",
        "      # These are three level UseSum9 cascades\n",
        "      [ 5555, 4445],\n",
        "      [ 55550, 44450],\n",
        "      [ 334, 666],\n",
        "      [ 3340, 6660],\n",
        "      [ 33400, 66600],\n",
        "      [ 888, 112],\n",
        "      [ 8880, 1120],\n",
        "      [ 88800, 11200],\n",
        "      [ 1234, 8766],\n",
        "      [ 4321, 5679],\n",
        "      # These are four level UseSum9 cascades\n",
        "      [ 44445, 55555],\n",
        "      [ 33334, 66666],\n",
        "      [ 88888, 11112],\n",
        "      [ 12345, 87655],\n",
        "      [ 54321, 45679],\n",
        "      [ 45545, 54455],\n",
        "      [ 36634, 63366],\n",
        "      [ 81818, 18182],\n",
        "      [ 87345, 12655],\n",
        "      [ 55379, 44621]])\n",
        "\n",
        "\n",
        "# These questions focus mainly on 1 digit at a time\n",
        "# (We're assuming that the 0 + 0 digit additions are trivial bigrams)\n",
        "def make_answerdigit_questions():\n",
        "    return make_questions(\n",
        "      [[ 1, 0],\n",
        "      [ 4, 3],\n",
        "      [ 5, 5],\n",
        "      [ 8, 1],\n",
        "      [ 40, 30],\n",
        "      [ 44, 46],\n",
        "      [ 400, 300],\n",
        "      [ 440, 460],\n",
        "      [ 800, 100],\n",
        "      [ 270, 470],\n",
        "      [ 600, 300],\n",
        "      [ 4000, 3000],\n",
        "      [ 4400, 4600],\n",
        "      [ 6000, 3000],\n",
        "      [ 7000, 4000],\n",
        "      [ 40000, 30000],\n",
        "      [ 44000, 46000],\n",
        "      [ 60000, 30000],\n",
        "      [ 70000, 40000],\n",
        "      [ 10000, 20000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 76000, 99000]])\n",
        "\n",
        "\n",
        "# Returns 128 random and ~100 manually-chosen questions\n",
        "def make_varied_questions():\n",
        "  q0, _, _, _, _, _ = next(ds)\n",
        "  q1 = make_ba_questions()\n",
        "  q2 = make_uc1_questions()\n",
        "  q3 = make_simple_us9_questions()\n",
        "  q4 = make_cascade_us9_questions()\n",
        "  q5 = make_answerdigit_questions()\n",
        "  q6, _, _, _, _, _ = next(ds)\n",
        "\n",
        "  questions = torch.vstack((q0.cuda(), q1.cuda(), q2.cuda(), q3.cuda(), q4.cuda(), q5.cuda(), q6.cuda()))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1cLLNpU3I0j"
      },
      "outputs": [],
      "source": [
        "# Test that the get_question_case code works as expected\n",
        "def unit_test_get_question_case_core(correct_case, questions):\n",
        "  num_questions = questions.shape[0]\n",
        "  print( correct_case, \"#Questions=\", num_questions)\n",
        "  for i in range(num_questions):\n",
        "    question_case = get_question_case(questions[i])\n",
        "    if question_case != correct_case:\n",
        "      print( \"Case mismatch:\", correct_case, question_case, questions[i])\n",
        "\n",
        "def unit_test_get_question_case():\n",
        "  unit_test_get_question_case_core( \"BA\", make_ba_questions())\n",
        "  unit_test_get_question_case_core( \"MC1\", make_uc1_questions())\n",
        "  unit_test_get_question_case_core( \"SimpleUS9\", make_simple_us9_questions())\n",
        "  unit_test_get_question_case_core( \"CascadeUS9\", make_cascade_us9_questions())\n",
        "\n",
        "unit_test_get_question_case()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4bqJ5C23M-A"
      },
      "outputs": [],
      "source": [
        "verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJayaDiR3PgJ"
      },
      "outputs": [],
      "source": [
        "# Build a test batch of 64 random and ~100 manually-chosen questions\n",
        "varied_questions = make_varied_questions();\n",
        "\n",
        "\n",
        "# Run the sample batch, gather the cache\n",
        "model.reset_hooks()\n",
        "model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = model.run_with_cache(varied_questions.cuda())\n",
        "print(sample_cache) # Gives names of datasets in the cache\n",
        "sample_losses_raw, sample_max_indices = logits_to_tokens_loss(sample_logits, varied_questions.cuda())\n",
        "sample_loss_mean = utils.to_numpy(loss_fn(sample_losses_raw).mean())\n",
        "print(\"Sample Mean Loss\", sample_loss_mean) # Loss < 0.04 is good\n",
        "\n",
        "\n",
        "# attn.hook_z is the \"attention head output\" hook point name (at a specified layer)\n",
        "l_attn_hook_z_name = [utils.get_act_name('z', 0, 'a'),utils.get_act_name('z', 1, 'a')] # 'blocks.0.attn.hook_z' etc\n",
        "sample_attn_z = sample_cache[l_attn_hook_z_name[0]]\n",
        "print(\"Sample\", l_attn_hook_z_name[0], sample_attn_z.shape) # gives [239, 18, 3, 170] = #questions, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "mean_attn_z = torch.mean(sample_attn_z, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_attn_hook_z_name[0], mean_attn_z.shape) # gives [1, 18, 3, 170] = 1, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "\n",
        "\n",
        "# hook_resid_pre is the \"pre residual memory update\" hook point name (at a specified layer)\n",
        "l_hook_resid_pre_name = ['blocks.0.hook_resid_pre','blocks.1.hook_resid_pre']\n",
        "\n",
        "\n",
        "# hook_resid_post is the \"post residual memory update\" hook point name (at a specified layer)\n",
        "l_hook_resid_post_name = ['blocks.0.hook_resid_post','blocks.1.hook_resid_post']\n",
        "sample_resid_post = sample_cache[l_hook_resid_post_name[0]]\n",
        "print(\"Sample\", l_hook_resid_post_name[0], sample_resid_post.shape) # gives [239, 18, 510] = #questions, cfg.n_ctx, cfg.d_model\n",
        "mean_resid_post = torch.mean(sample_resid_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_hook_resid_post_name[0], mean_resid_post.shape) # gives [1, 18, 510] = 1, cfg.n_ctx, cfg.d_model\n",
        "\n",
        "\n",
        "# mlp.hook_post is the \"MLP layer\" hook point name (at a specified layer)\n",
        "l_mlp_hook_post_name = [utils.get_act_name('post', 0),utils.get_act_name('post', 1)] # 'blocks.0.mlp.hook_post' etc\n",
        "sample_mlp_hook_post = sample_cache[l_mlp_hook_post_name[0]]\n",
        "print(\"Sample\", l_mlp_hook_post_name[0], sample_mlp_hook_post.shape) # gives [239, 18, 2040] = #questions, cfg.n_ctx, cfg.d_mlp\n",
        "mean_mlp_hook_post = torch.mean(sample_mlp_hook_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_mlp_hook_post_name[0], mean_mlp_hook_post.shape) # gives [1, 18, 2040] = 1, cfg.n_ctx, cfg.d_mlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGhK1OC_1mD"
      },
      "source": [
        "# Part 9: Attention Patterns\n",
        "Attention patterns show which token(s) the model's attention heads are paying attention to in each token position of the prediction calculation.\n",
        "\n",
        "For the default CoLab set up, the  model has 3 attention heads, and performs 5 digit addition. The attention pattern is 18 by 18 squares (as 54321+77779=132100 is 18 tokens). Time proceeds vertically downwards, with one additional token being revealed horizontally at each token position, giving the overall triangle shape. This visualisation provided insights. After the question is fully revealed (at token position 11), each head starts attending to pairs of question digits from left to right (i.e. high-value digits before lower-value digits) giving the “double staircase\" shape. The three heads attend to a given digit pair in three different token position, giving a time ordering of heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiVCSBSE3Xpk"
      },
      "outputs": [],
      "source": [
        "def show_token_attention_patterns(index, layer, token_at_index, use_case):\n",
        "\n",
        "  the_tokens = [str(token) for token in token_at_index.tolist()]\n",
        "  if layer == 0:\n",
        "    tokens_str = tokens_to_string(the_tokens)\n",
        "    print(\"Attention patterns for\", tokens_str)\n",
        "\n",
        "  attention_pattern=sample_cache[\"pattern\", layer, \"attn\"][index]\n",
        "  display(cv.attention.attention_patterns(\n",
        "      tokens=the_tokens,\n",
        "      attention=attention_pattern,\n",
        "      #attention_head_names=[f\"L{layer}H{i}\" for i in range(cfg.n_heads)],\n",
        "  ))\n",
        "\n",
        "\n",
        "sample_size = 3\n",
        "\n",
        "# Show attention patterns for some randomly chosen tokens\n",
        "for i in range(sample_size):\n",
        "  for layer in range(cfg.n_layers):\n",
        "    show_token_attention_patterns(i, layer, tokens[i], \"Misc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUCXw1k93a20"
      },
      "outputs": [],
      "source": [
        "if cfg.save_graph_to_file:\n",
        "\n",
        "  tokens_str = []\n",
        "  for i in range(cfg.n_heads):\n",
        "    one_token_str = []\n",
        "    for j in tokens[i]:\n",
        "      one_token_str.append(str(utils.to_numpy(j)))\n",
        "    tokens_str.append(one_token_str)\n",
        "\n",
        "  # Refer https://github.com/callummcdougall/CircuitsVis/blob/main/python/circuitsvis/circuitsvis_demo.ipynb\n",
        "\n",
        "  # html_object = cv.attention.from_cache(\n",
        "  #    cache = sample_cache,\n",
        "  #    tokens = tokens_str, # list of list of strings\n",
        "  #    return_mode = \"html\",\n",
        "  #)\n",
        "\n",
        "  # Create a CoLab file containing the attention pattern(s) in HTML\n",
        "  #filename = \"AttentionPattern\" + str(cfg.n_digits) + \"Digits\" + str(cfg.n_heads) + \"Heads.html\"\n",
        "  #with open(filename, \"w\") as f:\n",
        "  #    f.write(html_object.data)\n",
        "\n",
        "  # Manually download the CoLab \"html\" file and open in your local browser.\n",
        "  # Install and use the Edge extension \"FireShot\" to save a portion of the HTML page as a PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1vfpD1Z_6vK"
      },
      "source": [
        "# Part 10: Run a prediction question\n",
        "\n",
        "Create way to get model to predict sample question answers and analysis/show results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2tcTkbi3l5M"
      },
      "outputs": [],
      "source": [
        "def predict_experiment_question(questions, the_hook, the_threshold):\n",
        "\n",
        "  c_loss_mean = 0\n",
        "\n",
        "  for question_num in range(questions.shape[0]):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    model.set_use_attn_result(True)\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    c_loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "\n",
        "  return c_loss_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFTPgtnGA2U3"
      },
      "source": [
        "# Part 11 : Is the model 100% accurate?\n",
        "\n",
        "This is hard to prove. If it does 1M predictions without error, then we assume it is 100%.\n",
        "\n",
        "This part takes ~135 minutes to run for 5-digit 2-layer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia1CqJ5n4zXD"
      },
      "outputs": [],
      "source": [
        "def null_hook(value, hook):\n",
        "  global verbose\n",
        "\n",
        "  verbose = False\n",
        "\n",
        "\n",
        "def one_million_questions():\n",
        "  global verbose\n",
        "\n",
        "  verbose = False\n",
        "  the_threshold = 0.0001\n",
        "  num_successes = 0\n",
        "  num_fails = 0\n",
        "\n",
        "  num_batches = 1000000//cfg.batch_size\n",
        "  for epoch in range(num_batches):\n",
        "      tokens, _, _, _, _, _ = next(ds)\n",
        "\n",
        "      the_hook = [(l_attn_hook_z_name[0], null_hook)]\n",
        "      loss_mean = predict_experiment_question(tokens, the_hook, the_threshold)\n",
        "\n",
        "      if loss_mean > the_threshold:\n",
        "        break\n",
        "\n",
        "      num_successes = num_successes + cfg.batch_size\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "          print(\"Batch\", epoch, \"of\", num_batches, \"#Successes=\", num_successes)\n",
        "\n",
        "  print(\"successes\", num_successes, \"num_fails\", num_fails)\n",
        "\n",
        "\n",
        "# Commented out as it takes ~135 minutes to run with cfg.n_layers=2, n_digits=5, train=30K\n",
        "# one_million_questions()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Pg267eav_QSl",
        "LiOdvQFD_fxZ",
        "deGhK1OC_1mD",
        "j1vfpD1Z_6vK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
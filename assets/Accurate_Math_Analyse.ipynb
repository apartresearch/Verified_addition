{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG2gZSoSJD5C"
      },
      "source": [
        "# Accurate Integer Mathematics in Transformers - Analyse the Model\n",
        "\n",
        "This CoLab analyses a Transformer model that performs integer addition, subtraction and multiplication e.g. 133357+182243=+0315600, 123450-345670=-0123230 and 000345*000823=+283935. Each digit is a separate token. For 6 digit questions, the model is given 14 \"question\" (input) tokens, and must then predict the corresponding 8 \"answer\" (output) tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzkGrSqHJKqN"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTd3nmsMJV5T"
      },
      "source": [
        "# Part 0: Import libraries\n",
        "Imports standard libraries. Don't bother reading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCdmr6-_Jkzi",
        "outputId": "7b133a19-5da7-4ff5-a93e-b9515a922fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformer_lens\n",
            "  Downloading transformer_lens-1.14.0-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.23.0 (from transformer_lens)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer_lens)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Collecting numpy>=1.24 (from transformer_lens)\n",
            "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.0)\n",
            "Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer_lens)\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m674.1/755.5 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    !pip install matplotlib\n",
        "    !pip install prettytable\n",
        "\n",
        "    !pip install kaleido\n",
        "    !pip install transformer_lens\n",
        "    !pip install torchtyping\n",
        "    !pip install transformers\n",
        "\n",
        "    !pip install numpy --upgrade\n",
        "    !pip install scikit-learn --upgrade\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up2QLAZLJnG9"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve-TndERJoaJ"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6zOEFryJqGN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from prettytable import PrettyTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6TE7A9SxySA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import textwrap\n",
        "\n",
        "# Use Principal Component Analysis (PCA) library\n",
        "use_pca = True\n",
        "try:\n",
        "  from sklearn.decomposition import PCA\n",
        "except Exception as e:\n",
        "  print(\"pca import exception:\", e)\n",
        "  use_pca = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8VQ4e0QJsIB"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQNjIosyX9Y-"
      },
      "source": [
        "# Part 1A: Configuration: Detailed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwXrSj8KGj-v"
      },
      "outputs": [],
      "source": [
        "# Tokens used in vocab. (Token indexes 0 to 9 represent digits 0 to 9)\n",
        "PLUS_INDEX = 10\n",
        "MINUS_INDEX = 11\n",
        "EQUALS_INDEX = 12\n",
        "MULT_INDEX = 13\n",
        "DIV_INDEX = 14\n",
        "MAX_INDEX = DIV_INDEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmjGdFcdJat3"
      },
      "outputs": [],
      "source": [
        "# Main configuration class for main model creation and training\n",
        "class Config():\n",
        "  #@markdown Main Model\n",
        "  n_layers: int = 3 #@param\n",
        "  n_heads: int = 4 #@param\n",
        "\n",
        "  d_vocab: int = MAX_INDEX+1\n",
        "  d_model: int = 510\n",
        "  d_mlp: int = 4 * d_model\n",
        "  d_head: int = 170\n",
        "  training_seed: int = 372001\n",
        "  analysis_seed: int = 673023\n",
        "\n",
        "  #@markdown Data\n",
        "  n_digits: int = 6 #@param\n",
        "  n_ctx: int = 3 * n_digits + 4\n",
        "  act_fn: str = 'relu'\n",
        "  batch_size: int = 512 # Training used 64. Larger for speed during analysis\n",
        "\n",
        "  #@markdown Optimizer\n",
        "  n_training_steps: int = 40000 #@param\n",
        "  weight_decay: float = 0.00008\n",
        "  lr: int = 0.1\n",
        "\n",
        "  #@markdown Actions\n",
        "\n",
        "  # Percent of questions that are multiplication, subtraction (rest are addition questions).\n",
        "  perc_mult: int = 0 # e.g. 20\n",
        "  perc_sub: int = 80 #@param e.g. 80\n",
        "  def perc_add(self):\n",
        "    return max(0, 100 - self.perc_mult - self.perc_sub)\n",
        "\n",
        "  #@markdown Insert Model\n",
        "  insert_mode: int = 0 #@param 0=None 1=Init, 2=FreezeHeads 3=FreezeAll\n",
        "\n",
        "\n",
        "  # Save graphs to CoLab temp files as PDF, SVG, etc. You can manually export files for re-use in papers.\n",
        "  save_graph_to_file: bool = True\n",
        "\n",
        "  # The format to output prettytable in. Options are text|html|json|csv|latex\n",
        "  # Use Text for this CoLab, latex for Overleaf output, and html for GitHub blog output\n",
        "  table_out_format: str = \"text\"\n",
        "\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGPkaokJQM5"
      },
      "source": [
        "# Part 1B: Configuration: Summary\n",
        "\n",
        "Which existing model do we want to analyse?\n",
        "\n",
        "The existing model weightings created by the sister Colab [Accurate_Math_Train](https://github.com/PhilipQuirke/transformer-maths/blob/main/assets/Accurate_Math_Train.ipynb) are loaded from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1DXZQ2E6yAi"
      },
      "outputs": [],
      "source": [
        "# Which existing model do we want to analyse?\n",
        "#model_name = \"add_d5_l2_h3_train15K\"  # 5 digit addition model\n",
        "model_name = \"add_d6_l2_h3_train15K\"  # 6 digit addition model\n",
        "#model_name = \"sub_d6_l2_h3_train30K\"  # 6 digit subtraction model\n",
        "#model_name = \"mix_d6_l3_h4_train40K\"  # 6 digit addition and subtraction model. AvgFinalLoss=8e-09\n",
        "#model_name = \"ins1_mix_d6_l3_h4_train40K\"  # 6 digit addition / subtraction model. Initialise with addition model. Handles 1m Qs for Add and Sub\n",
        "#model_name = \"ins2_mix_d6_l4_h4_train40K\"  # 6 digit addition / subtraction model. Initialised with addition model. Reset useful heads every 100 epochs. AvgFinalLoss=7e-09. Fails 1m Qs\n",
        "#model_name = \"ins3_mix_d6_l4_h3_train40K\"  # 6 digit addition / subtraction model. Initialised with addition model. Reset useful heads & MLPs every 100 epochs. AvgFinalLoss=2.6e-06. Fails 1m Qs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5BiELcf53ms"
      },
      "outputs": [],
      "source": [
        "if model_name == \"add_d5_l2_h3_train15K\" :\n",
        "  cfg.n_digits = 5\n",
        "  cfg.n_layers = 2\n",
        "  cfg.n_heads = 3\n",
        "  cfg.n_training_steps = 15000\n",
        "  cfg.perc_sub = 0\n",
        "  cfg.insert_mode = 0\n",
        "\n",
        "if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "  cfg.n_digits = 6\n",
        "  cfg.n_layers = 2\n",
        "  cfg.n_heads = 3\n",
        "  cfg.n_training_steps = 15000\n",
        "  cfg.perc_sub = 0\n",
        "  cfg.insert_mode = 0\n",
        "\n",
        "if model_name == \"sub_d6_l2_h3_train30K\" :\n",
        "  cfg.n_digits = 6\n",
        "  cfg.n_layers = 2\n",
        "  cfg.n_heads = 3\n",
        "  cfg.n_training_steps = 30000\n",
        "  cfg.perc_sub = 100\n",
        "  cfg.insert_mode = 0\n",
        "\n",
        "if model_name == \"mix_d6_l3_h4_train40K\" :\n",
        "  cfg.n_digits = 6\n",
        "  cfg.n_layers = 3\n",
        "  cfg.n_heads = 4\n",
        "  cfg.batch_size = 256\n",
        "  cfg.n_training_steps = 40000\n",
        "  cfg.perc_sub = 66 # Train on 66% subtraction and 33% addition question batches\n",
        "  cfg.insert_mode = 0\n",
        "\n",
        "if model_name == \"ins1_mix_d6_l3_h4_train40K\" :\n",
        "  cfg.n_digits = 6\n",
        "  cfg.n_layers = 3\n",
        "  cfg.n_heads = 4\n",
        "  cfg.n_training_steps = 40000\n",
        "  cfg.batch_size = 256\n",
        "  cfg.perc_sub = 80 # Train on 80% subtraction and 20% addition question batches\n",
        "  cfg.insert_mode = 1 # Initialise with add_d6_l2_h3_train15K.pth.\n",
        "\n",
        "if model_name == \"ins2_mix_d6_l4_h4_train40K\" :\n",
        "  cfg.n_digits = 6\n",
        "  cfg.n_layers = 4\n",
        "  cfg.n_heads = 4\n",
        "  cfg.n_training_steps = 40000\n",
        "  cfg.batch_size = 256\n",
        "  cfg.perc_sub = 80 # Train on 80% subtraction and 20% addition question batches\n",
        "  cfg.insert_mode = 2 # Initialise with add_d6_l2_h3_train15K.pth. Train & reset useful heads every 100 epochs\n",
        "\n",
        "if model_name == \"ins3_mix_d6_l4_h3_train40K\" :\n",
        "  cfg.n_digits = 6\n",
        "  cfg.n_layers = 4\n",
        "  cfg.n_heads = 3\n",
        "  cfg.n_training_steps = 40000\n",
        "  cfg.batch_size = 256\n",
        "  cfg.perc_sub = 80 # Train on 80% subtraction and 20% addition question batches\n",
        "  cfg.insert_mode = 3 # Initialise with add_d6_l2_h3_train15K.pth. Trained & reset useful heads & MLPs every 100 epochs\n",
        "\n",
        "cfg.n_ctx = 3 * cfg.n_digits + 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0DJkn5l2gq3"
      },
      "outputs": [],
      "source": [
        "def file_name_suffix(digits, layers, heads, training_steps, seed):\n",
        "  train_str = str(training_steps//1000) + \"K\"\n",
        "  return '_d{}_l{}_h{}_train{}_seed{}'.format(digits, layers, heads, train_str, seed)\n",
        "\n",
        "op_prefix = 'mul' if cfg.perc_mult == 100 else 'sub' if cfg.perc_sub == 100 else 'add' if cfg.perc_add() == 100 else 'mix'\n",
        "main_fname = '' if cfg.insert_mode == 0 else 'ins{}_'.format(cfg.insert_mode)\n",
        "main_fname += op_prefix\n",
        "main_fname += file_name_suffix(cfg.n_digits, cfg.n_layers, cfg.n_heads, cfg.n_training_steps, cfg.training_seed)\n",
        "main_fname_pth = main_fname + '.pth'\n",
        "main_fname_json = main_fname + '_tags.json'\n",
        "\n",
        "\n",
        "def print_config():\n",
        "  print(\"%Mult=\", cfg.perc_mult, \"%Sub=\", cfg.perc_sub, \"%Add=\", cfg.perc_add(), \"File=\", main_fname)\n",
        "\n",
        "print_config()\n",
        "print('Main model be read from HuggingLab file', main_fname_pth)\n",
        "print('Main model quanta tags will save to Colab temporary file', main_fname_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8RfHXneJw6n"
      },
      "source": [
        "# Part 3: Set Up: Create model\n",
        "This section defines the token embedding / unembedding and creates the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QYFZIalJ3tK"
      },
      "outputs": [],
      "source": [
        "# Embedding / Unembedding\n",
        "\n",
        "def token_to_char(i):\n",
        "  if i < 10:\n",
        "   return str(i)\n",
        "  if i == PLUS_INDEX:\n",
        "    return \"+\"\n",
        "  if i == MINUS_INDEX:\n",
        "    return \"-\"\n",
        "  if i == EQUALS_INDEX:\n",
        "    return \"=\"\n",
        "  if i == MULT_INDEX:\n",
        "    return \"*\"\n",
        "  if i == DIV_INDEX:\n",
        "    return \"\\\\\"\n",
        "  return \"?\"\n",
        "\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    tokens = utils.to_numpy(tokens)\n",
        "    return \"\".join([token_to_char(i) for i in tokens[:cfg.n_ctx]])\n",
        "\n",
        "\n",
        "def string_to_tokens(string, batch: bool=False):\n",
        "    lookup = {str(i):i for i in range(10)}\n",
        "    lookup['+']=PLUS_INDEX\n",
        "    lookup['-']=MINUS_INDEX\n",
        "    lookup['=']=EQUALS_INDEX\n",
        "    lookup['*']=MULT_INDEX\n",
        "    lookup['\\\\']=DIV_INDEX\n",
        "\n",
        "    tokens = [lookup[i] for i in string if i not in '\\n ']\n",
        "    if batch:\n",
        "        return torch.tensor(tokens)[None, :]\n",
        "    else:\n",
        "        return torch.tensor(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA16Nb2PJ7MB"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = HookedTransformerConfig(\n",
        "    n_layers = cfg.n_layers,\n",
        "    n_heads = cfg.n_heads,\n",
        "    d_model = cfg.d_model,\n",
        "    d_head = cfg.d_head,\n",
        "    d_mlp = cfg.d_mlp,\n",
        "    act_fn = cfg.act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab = cfg.d_vocab,\n",
        "    d_vocab_out = cfg.d_vocab,\n",
        "    n_ctx = cfg.n_ctx,\n",
        "    init_weights = True,\n",
        "    device = \"cuda\",\n",
        "    seed = cfg.training_seed,\n",
        ")\n",
        "\n",
        "main_model = HookedTransformer(ht_cfg)\n",
        "\n",
        "optimizer = torch.optim.AdamW(main_model.parameters(),\n",
        "                        lr = cfg.lr,\n",
        "                        weight_decay = cfg.weight_decay,\n",
        "                        betas = (0.9, 0.98))\n",
        "\n",
        "max_iter = cfg.n_training_steps\n",
        "warmup_iter = max_iter // 5\n",
        "scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=int(warmup_iter))\n",
        "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(np.ceil((max_iter-warmup_iter))))\n",
        "scheduler  = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[int(warmup_iter)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiJhch4KCej"
      },
      "source": [
        "# Part 4: Set Up: Loss Function & Data Generator\n",
        "This section defines the loss function and the training/tesing data generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ2iNO-nKDBW"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "\n",
        "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
        "def logits_to_tokens_loss(logits, tokens):\n",
        "  # Addition answer can have one extra digit than question. Answer also has a +/- sign\n",
        "  n_answer_digits = cfg.n_digits+2\n",
        "\n",
        "  # The addition answer digit token probabilities\n",
        "  ans_logits = logits[:, -(n_answer_digits+1):-1]\n",
        "\n",
        "  # Convert raw score (logits) vector into a probability distribution.\n",
        "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
        "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
        "\n",
        "  max_prob_tokens = torch.argmax(ans_probs, dim=-1)\n",
        "\n",
        "  # The addition answer digit tokens\n",
        "  ans_tokens = tokens[:, -(n_answer_digits):]\n",
        "\n",
        "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
        "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
        "\n",
        "  return ans_loss, max_prob_tokens\n",
        "\n",
        "\n",
        "# Calculate loss as negative of average per-token mean probability\n",
        "def loss_fn(ans_loss):\n",
        "  return -ans_loss.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUobUMfCkLcs"
      },
      "outputs": [],
      "source": [
        "# Generate an enriched data batch for one operator type\n",
        "# \"Addition\" batch entries are formated XXXXX+YYYYY=+ZZZZZZ e.g. 550030+800020=+1350050\n",
        "# \"Subtraction\" batch entries are formated XXXXX-YYYYY=-ZZZZZZ e.g. 550030-800020=-0249990, 800020-550030=+0249990\n",
        "# \"Multiplication\" batch entries are formated 000XXX*000YYY=+ZZZZZZ e.g. 000345*000678=+233910\n",
        "def data_generator_core( batch_op ):\n",
        "\n",
        "  batch = torch.zeros((cfg.batch_size, cfg.n_ctx)).to(torch.int64)\n",
        "  x = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "  y = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "\n",
        "  if batch_op == MULT_INDEX:\n",
        "    # Convert from NNNNNN*NNNNNN= to 000NNN*000NNN= so answer (product) is NNNNNN\n",
        "    num_zeros = cfg.n_digits // 2\n",
        "    for z in range(num_zeros):\n",
        "      x[:, z] = 0\n",
        "      y[:, z] = 0\n",
        "\n",
        "  # Enrich the question data on 60% of batches to speed up training\n",
        "  if ( batch_op == PLUS_INDEX or batch_op == MINUS_INDEX ) and (random.randint(1, 5) < 3):\n",
        "    # Flatten x and y to 1D tensors\n",
        "    x_flat = x.view(-1)\n",
        "    y_flat = y.view(-1)\n",
        "\n",
        "    if batch_op == PLUS_INDEX :\n",
        "      # The UseSum9 task is compound and rare and so hard to learn.\n",
        "      # Increase the MakeSum9 case frequency\n",
        "      # UseSum9 also relies on MakeCarry1 (50%) from previous column.\n",
        "      num_elements_to_modify = int(0.40 * x.numel()) # 40%\n",
        "      indices_to_modify = torch.randperm(x_flat.numel())[:num_elements_to_modify]\n",
        "      if random.randint(1, 2) == 1:\n",
        "        x_flat[indices_to_modify] = 9 - y_flat[indices_to_modify]\n",
        "      else:\n",
        "        y_flat[indices_to_modify] = 9 - x_flat[indices_to_modify]\n",
        "    else:\n",
        "      # Empirically, the model seems to struggle with the sign calculation.\n",
        "      # Minus signs are rarer than positive signs.\n",
        "      # Generate more negative answers by increasing the y value\n",
        "      y_flat[y_flat < 9] += 1\n",
        "\n",
        "    # Reshape x and y back to its original shape\n",
        "    x = x_flat.view(x.shape)\n",
        "    y = y_flat.view(x.shape)\n",
        "\n",
        "\n",
        "  batch[:, :cfg.n_digits] = x\n",
        "  batch[:, cfg.n_digits] = batch_op\n",
        "  batch[:, 1+cfg.n_digits:1+cfg.n_digits*2] = y\n",
        "  batch[:, 1+cfg.n_digits*2] = EQUALS_INDEX\n",
        "\n",
        "  # Convert each row into a 5-digit number\n",
        "  x_values = x[:, 0]\n",
        "  y_values = y[:, 0]\n",
        "  for dn in range(1,cfg.n_digits):\n",
        "    x_values = x_values * 10 + x[:, dn]\n",
        "    y_values = y_values * 10 + y[:, dn]\n",
        "\n",
        "  # Elementwise operations to give the 1D tensor answers\n",
        "  if batch_op == MULT_INDEX:\n",
        "    answers = x_values * y_values\n",
        "  else:\n",
        "    if batch_op == MINUS_INDEX:\n",
        "      answers = x_values - y_values\n",
        "    else:\n",
        "      answers = x_values + y_values\n",
        "\n",
        "  # Insert the answers into the batch\n",
        "  for i in range(cfg.batch_size):\n",
        "    answer = answers[i]\n",
        "\n",
        "    sign = PLUS_INDEX\n",
        "    if answer < 0:\n",
        "      sign = MINUS_INDEX\n",
        "      answer = - answer\n",
        "\n",
        "    batch[i, 2+cfg.n_digits*2] = sign\n",
        "    for j in range(cfg.n_digits+1):\n",
        "      batch[i, cfg.n_ctx-j-1] = answer % 10\n",
        "      answer = answer // 10\n",
        "      if answer == 0:\n",
        "          break\n",
        "\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSp8pS1eKHf6"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" data generator function. Invoked using next().\n",
        "def data_generator( ):\n",
        "  torch.manual_seed(cfg.analysis_seed)\n",
        "  while True:\n",
        "\n",
        "    batch_rand = random.randint(1, 100)\n",
        "    batch_op = MULT_INDEX if batch_rand <= cfg.perc_mult else MINUS_INDEX if batch_rand <= cfg.perc_mult + cfg.perc_sub else PLUS_INDEX\n",
        "\n",
        "    batch = data_generator_core( batch_op )\n",
        "\n",
        "    yield batch.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqzljhQ4KJU5"
      },
      "outputs": [],
      "source": [
        "# Initialise the data generator\n",
        "ds = data_generator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtmioT1THbJA"
      },
      "outputs": [],
      "source": [
        "# Test data generator\n",
        "tokens = next(ds)\n",
        "print(tokens[:3,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KJhCxFtNKfm"
      },
      "source": [
        "# Part 5: Set Up: Load Model from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMkB_8GNRc0"
      },
      "outputs": [],
      "source": [
        "main_repo_name=\"PhilipQuirke/Accurate6DigitSubtraction\"\n",
        "print(\"Loading model from HuggingFace\", main_repo_name, main_fname_pth)\n",
        "\n",
        "main_model.load_state_dict(utils.download_file_from_hf(repo_name=main_repo_name, file_name=main_fname_pth, force_is_torch=True))\n",
        "main_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxoBWHNKRf0"
      },
      "source": [
        "# Part 6: Set Up: Sample Questions by Quanta\n",
        "\n",
        "Create sets of sample questions exercising different quanta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr4OZri5sq1S"
      },
      "outputs": [],
      "source": [
        "# Mathematical operations\n",
        "addition_major_tag = \"Add\"\n",
        "subtraction_major_tag = \"Sub\"\n",
        "multiplication_major_tag = \"Mult\"\n",
        "varied_major_tag = \"Varied\"\n",
        "\n",
        "# Answer digit impact\n",
        "impact_major_tag = \"Impact\"\n",
        "\n",
        "# Ablation failure percentage\n",
        "perc_major_tag = \"FailPerc\"\n",
        "\n",
        "# Attention pattern\n",
        "attention_major_tag = \"Attn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj_xSuSSKR9t"
      },
      "outputs": [],
      "source": [
        "# Insert a number into the question\n",
        "def insert_question_number(the_question, index, first_digit_index, the_digits, n):\n",
        "\n",
        "  last_digit_index = first_digit_index + the_digits - 1\n",
        "\n",
        "  for j in range(the_digits):\n",
        "    the_question[index, last_digit_index-j] = n % 10\n",
        "    n = n // 10\n",
        "\n",
        "\n",
        "# Create a single question\n",
        "def make_a_question(the_question, index, q1, q2, operator ):\n",
        "\n",
        "  insert_question_number(the_question, index, 0, cfg.n_digits, q1)\n",
        "\n",
        "  the_question[index, cfg.n_digits] = operator\n",
        "\n",
        "  insert_question_number( the_question, index, cfg.n_digits+1, cfg.n_digits, q2)\n",
        "\n",
        "  the_question[index, 2*cfg.n_digits+1] = EQUALS_INDEX\n",
        "\n",
        "  answer = q1+q2\n",
        "  if operator == MINUS_INDEX:\n",
        "    answer = q1-q2\n",
        "  else:\n",
        "    if operator == MULT_INDEX:\n",
        "      answer = q1*q2\n",
        "\n",
        "  the_question[index, 2*cfg.n_digits+2] = PLUS_INDEX if answer >= 0 else MINUS_INDEX\n",
        "  if answer < 0:\n",
        "    answer = -answer\n",
        "\n",
        "  insert_question_number(the_question, index, 2*cfg.n_digits + 3, cfg.n_digits+1, answer)\n",
        "\n",
        "\n",
        "# Create a batch of questions from a 2D matrix of ints\n",
        "def make_questions(operator, q_matrix):\n",
        "  max_len = len(q_matrix)\n",
        "  real_len = 0\n",
        "  questions = torch.zeros((max_len, cfg.n_ctx)).to(torch.int64)\n",
        "  limit = 10 ** cfg.n_digits\n",
        "\n",
        "  for i in range(max_len):\n",
        "    a = q_matrix[i][0]\n",
        "    b = q_matrix[i][1]\n",
        "\n",
        "    if a < limit and b < limit:\n",
        "      make_a_question(questions, real_len, a, b, operator)\n",
        "      real_len += 1\n",
        "\n",
        "  return questions[:real_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRCPyETTKaEs"
      },
      "outputs": [],
      "source": [
        "# Manually create some questions that strongly test one quanta\n",
        "\n",
        "\n",
        "# Make BaseAdd questions\n",
        "def make_s0_questions():\n",
        "    return addition_major_tag, \"S0\", make_questions( PLUS_INDEX,\n",
        "      [[0, 0],\n",
        "      [1, 3],\n",
        "      [12345, 33333],\n",
        "      [33333, 12345],\n",
        "      [45762, 33113],\n",
        "      [888, 11111],\n",
        "      [2362, 23123],\n",
        "      [15, 81],\n",
        "      [1000, 4441],\n",
        "      [4440, 11111],\n",
        "      [24033, 25133],\n",
        "      [23533, 21133],\n",
        "      [32500, 1],\n",
        "      [31500, 1111],\n",
        "      [5500, 12323],\n",
        "      [4500, 2209],\n",
        "      [33345, 66643], # =099988\n",
        "      [66643, 33345], # =099988\n",
        "      [10770, 44111],\n",
        "      [60000, 31111],\n",
        "      [10000, 21111],\n",
        "      [107700, 441111],\n",
        "      [600000, 311111],\n",
        "      [100000, 211111],\n",
        "      [1077000, 4411111],\n",
        "      [6000000, 3111111],\n",
        "      [1000000, 2111111],\n",
        "      [10770000, 44111111],\n",
        "      [60000000, 3111111],\n",
        "      [10000000, 2111111]])\n",
        "\n",
        "# Make UseCarry1 (addition) questions\n",
        "def make_s1_questions():\n",
        "    return addition_major_tag, \"S1\", make_questions( PLUS_INDEX,\n",
        "      [[ 15, 45],\n",
        "      [ 27, 55],\n",
        "      [ 35, 59],\n",
        "      [ 150, 451],\n",
        "      [ 270, 551],\n",
        "      [ 350, 591],\n",
        "      [ 1500, 4511],\n",
        "      [ 2700, 5511],\n",
        "      [ 3500, 5911],\n",
        "      [ 40035, 41149],\n",
        "      # [ 44000, 46000], D6 L1 H3 model cant handle this.\n",
        "      [ 70000, 41111],\n",
        "      [ 15000, 25111],\n",
        "      [ 35000, 35111],\n",
        "      [ 45000, 35111],\n",
        "      [ 67000, 25111],\n",
        "      [ 19000, 76111],\n",
        "      [ 15020, 45091],\n",
        "      [ 25002, 55019],\n",
        "      [ 35002, 59019],\n",
        "      [ 150211, 450911],\n",
        "      [ 250021, 550191],\n",
        "      [ 350021, 590191],\n",
        "      [ 1502111, 4509111],\n",
        "      [ 2500211, 5501911],\n",
        "      [ 3500211, 5901911],\n",
        "      [ 15021111, 45091111],\n",
        "      [ 25002111, 55019111],\n",
        "      [ 35002111, 59019111]])\n",
        "\n",
        "\n",
        "# Make SimpleUseSum9 (addition) questions\n",
        "def make_s2_questions():\n",
        "    return addition_major_tag,\"S2\", make_questions( PLUS_INDEX,\n",
        "      [[ 55, 45],\n",
        "      [ 45, 55],\n",
        "      [ 45, 59],\n",
        "      [ 35, 69],\n",
        "      [ 25, 79],\n",
        "      [ 15, 85],\n",
        "      [ 15, 88],\n",
        "      [ 15518, 14511],\n",
        "      [ 14518, 15511],\n",
        "      [ 24533, 25933],\n",
        "      [ 23533, 26933],\n",
        "      [ 32511, 7911],\n",
        "      [ 31511, 8511],\n",
        "      [ 551, 451],\n",
        "      [ 451, 551],\n",
        "      [ 10881, 41127],\n",
        "      [ 41127, 10881],\n",
        "      [ 12386, 82623],\n",
        "      [ 108811, 411271],\n",
        "      [ 411271, 108811],\n",
        "      [ 123861, 826231],\n",
        "      [ 994890, 80105],\n",
        "      [ 970590, 96026],\n",
        "      [ 994890, 80105],\n",
        "      [ 970590, 96026],\n",
        "      [ 1088111, 4112711],\n",
        "      [ 4112711, 1088111],\n",
        "      [ 1238611, 8262311],\n",
        "      [ 10881111, 41127111],\n",
        "      [ 41127111, 10881111],\n",
        "      [ 12386111, 82623111]])\n",
        "\n",
        "# These are two level UseSum9 cascades\n",
        "def make_s3_questions():\n",
        "    return addition_major_tag, \"S3\", make_questions( PLUS_INDEX,\n",
        "      [[ 555, 445],\n",
        "      [ 3340, 6661],\n",
        "      [ 8880, 1121],\n",
        "      [ 1120, 8881],\n",
        "      [ 123, 877],\n",
        "      [ 877, 123],\n",
        "      [ 321, 679],\n",
        "      [ 679, 321],\n",
        "      [ 1283, 78785]])\n",
        "\n",
        "\n",
        "# These are three level UseSum9 cascades\n",
        "def make_s4_questions():\n",
        "    return addition_major_tag, \"S4\", make_questions( PLUS_INDEX,\n",
        "      [[ 5555, 4445],\n",
        "      [ 55550, 44451],\n",
        "      [ 3334, 6666],\n",
        "      [ 33340, 66661],\n",
        "      [ 8888, 1112],\n",
        "      [ 88880, 11121],\n",
        "      [ 1234, 8766],\n",
        "      [ 4321, 5679]])\n",
        "\n",
        "\n",
        "# These are four level UseSum9 cascades\n",
        "def make_s5_questions():\n",
        "    return addition_major_tag, \"S5\", make_questions( PLUS_INDEX,\n",
        "      [[ 44445, 55555],\n",
        "      [ 33334, 66666],\n",
        "      [ 88888, 11112],\n",
        "      [ 12345, 87655],\n",
        "      [ 54321, 45679],\n",
        "      [ 45545, 54455],\n",
        "      [ 36634, 63366],\n",
        "      [ 81818, 18182],\n",
        "      [ 87345, 12655],\n",
        "      [ 55379, 44621]])\n",
        "\n",
        "\n",
        "# Make questions focus mainly on 1 digit at a time\n",
        "# (assuming that the 0 + 0 digit additions/subtractions are trivial bigrams)\n",
        "def make_sn_questions():\n",
        "    return addition_major_tag, \"SN\", make_questions( PLUS_INDEX,\n",
        "      [[ 1, 0],\n",
        "      [ 4, 3],\n",
        "      [ 5, 5],\n",
        "      [ 8, 1],\n",
        "      [ 40, 31],\n",
        "      [ 44, 46],\n",
        "      [ 400, 311],\n",
        "      [ 440, 461],\n",
        "      [ 800, 111],\n",
        "      [ 270, 471],\n",
        "      [ 600, 311],\n",
        "      [ 4000, 3111],\n",
        "      [ 4400, 4611],\n",
        "      [ 6000, 3111],\n",
        "      [ 7000, 4111],\n",
        "      [ 40000, 31111],\n",
        "      [ 44000, 45111],\n",
        "      [ 60000, 31111],\n",
        "      [ 70000, 41111],\n",
        "      [ 10000, 21111],\n",
        "      [ 15000, 25111],\n",
        "      [ 35000, 35111],\n",
        "      [ 45000, 85111],\n",
        "      [ 67000, 85111],\n",
        "      [ 99000, 76111],\n",
        "      [ 76000, 99111],\n",
        "      [ 670000, 851111],\n",
        "      [ 990000, 761111],\n",
        "      [ 760000, 991111],\n",
        "      [ 6700000, 8511111],\n",
        "      [ 9900000, 7611111],\n",
        "      [ 7600000, 9911111],\n",
        "      [ 67000000, 85111111],\n",
        "      [ 99000000, 76111111],\n",
        "      [ 76000000, 99111111]])\n",
        "\n",
        "\n",
        "# Make M0 questions - when no column generates a Borrow One. Answer is always positive (or zero).\n",
        "def make_m0_questions():\n",
        "    return subtraction_major_tag, \"M0\", make_questions( MINUS_INDEX,\n",
        "      [[0, 0],\n",
        "      [6, 6],\n",
        "      [61, 60],\n",
        "      [611, 600],\n",
        "      [6111, 6000],\n",
        "      [61111, 60000],\n",
        "      [611111, 600000],\n",
        "      [6111111, 6000000],\n",
        "      [61111111, 60000000],\n",
        "      [66666, 12345],\n",
        "      [33333, 12321],\n",
        "      [45762, 34551],\n",
        "      [78901, 78901], # = +000000\n",
        "      [23123, 23123], # = +000000\n",
        "      [86, 15],\n",
        "      [4440, 1230],\n",
        "      [88746, 86544],\n",
        "      [27833, 25133],\n",
        "      [23533, 21133],\n",
        "      [32501, 1],\n",
        "      [31511, 1111],\n",
        "      [55555, 12323],\n",
        "      [45454, 22022],\n",
        "      [66643, 3341],\n",
        "      [66643, 30042],\n",
        "      [99999, 44012],\n",
        "      [61111, 30000],\n",
        "      [99111, 99111], # = +000000\n",
        "      [999991, 440120],\n",
        "      [611111, 300000],\n",
        "      [991111, 991111], # = +0000000\n",
        "      [9999911, 4401200],\n",
        "      [6111111, 3000000],\n",
        "      [9911111, 9911111], # = +00000000\n",
        "      [99999111, 44012000],\n",
        "      [61111111, 30000000],\n",
        "      [99111111, 99111111]]) # = +000000000\n",
        "\n",
        "# Make subtraction M1 questions with exactly one \"borrow 1\" instance. Answer is always positive.\n",
        "def make_m1_questions():\n",
        "    return subtraction_major_tag, \"M1\", make_questions( MINUS_INDEX,\n",
        "      [[22222, 11113],\n",
        "      [ 22222, 11131],\n",
        "      [ 22222, 11311],\n",
        "      [ 22222, 13111],\n",
        "      [    14,     8],\n",
        "      [   141,    80],\n",
        "      [  1411,   800],\n",
        "      [ 14111,  8000],\n",
        "      [ 55514, 11108],\n",
        "      [ 55141, 11080],\n",
        "      [ 51411, 10800],\n",
        "      [ 140111,  8000],\n",
        "      [ 88888, 22229],\n",
        "      [ 77777, 22292],\n",
        "      [ 66666, 22922],\n",
        "      [ 888888, 222292],\n",
        "      [ 777777, 222922],\n",
        "      [ 666666, 229222],\n",
        "      [ 8888888, 2222922],\n",
        "      [ 7777777, 2229222],\n",
        "      [ 6666666, 2292222],\n",
        "      [ 88888888, 22229222],\n",
        "      [ 77777777, 22292222],\n",
        "      [ 66666666, 22922222]])\n",
        "\n",
        "# Make subtraction M2 questions containing B1 and DZ. Answer is always positive (or zero).\n",
        "def make_m2_questions():\n",
        "    return subtraction_major_tag, \"M2\", make_questions( MINUS_INDEX,\n",
        "      [[22212, 11113],\n",
        "      [ 22122, 11131],\n",
        "      [ 21222, 11311],\n",
        "      [   904,     8],\n",
        "      [  9041,    80],\n",
        "      [ 90411,   800],\n",
        "      [ 55514, 11118],\n",
        "      [ 55141, 11180],\n",
        "      [ 51411, 11800],\n",
        "      [ 88888, 22289],\n",
        "      [ 77777, 22792],\n",
        "      [ 66666, 26922],\n",
        "      [ 888888, 222892],\n",
        "      [ 777777, 227922],\n",
        "      [ 666666, 269222],\n",
        "      [ 8888888, 2228922],\n",
        "      [ 7777777, 2279222],\n",
        "      [ 6666666, 2692222],\n",
        "      [ 88888888, 22289222],\n",
        "      [ 77777777, 22792222],\n",
        "      [ 66666666, 26922222]])\n",
        "\n",
        "\n",
        "# Make subtraction M3,M4,... questions containing B1 and multiple DZs. Answer is always positive (or zero).\n",
        "def make_m3_questions():\n",
        "    return subtraction_major_tag, \"M3\", make_questions( MINUS_INDEX,\n",
        "      [[22112, 11113],\n",
        "      [ 21122, 11131],\n",
        "      [ 99004,     8],\n",
        "      [ 90041,    80],\n",
        "      [ 55114, 11118],\n",
        "      [ 51140, 11180],\n",
        "      [ 88888, 22889],\n",
        "      [ 87777, 27792],\n",
        "      [ 888888, 228892],\n",
        "      [ 877777, 277922],\n",
        "      [ 8888888, 2288922],\n",
        "      [ 7777777, 2779222],\n",
        "      [ 88888888, 22889222],\n",
        "      [ 77777777, 28892222]])\n",
        "\n",
        "\n",
        "# Make subtraction questions with negative answers\n",
        "def make_ng_questions():\n",
        "    return subtraction_major_tag, \"NG\", make_questions( MINUS_INDEX,\n",
        "      [[0, 1],\n",
        "      [7, 9],\n",
        "      [12345, 33333],\n",
        "      [888, 11111],\n",
        "      [2362, 23123],\n",
        "      [15, 81],\n",
        "      [1111, 4440],\n",
        "      [24033, 25133],\n",
        "      [23533, 88133],\n",
        "      [5511, 12323],\n",
        "      [4511, 22209],\n",
        "      [ 88888, 88889],\n",
        "      [ 55555, 55556],\n",
        "      [ 88881, 88891],\n",
        "      [ 55551, 55561],\n",
        "      [ 88811, 88911],\n",
        "      [ 55511, 55611],\n",
        "      [ 88746, 89544],\n",
        "      [ 27833, 29133],\n",
        "      [ 23533, 23833],\n",
        "      [ 31511, 41111],\n",
        "      [ 55555, 62323],\n",
        "      [ 45454, 72022],\n",
        "      [ 66643, 73341],\n",
        "      [ 66643, 90042],\n",
        "      [ 99998, 99999],\n",
        "      [ 8, 12],\n",
        "      [ 41, 232],\n",
        "      [ 44, 523],\n",
        "      [ 234, 334],\n",
        "      [ 7777, 8434],\n",
        "      [ 88888, 92222],\n",
        "      [ 77777, 84340],\n",
        "      [ 888888, 922220],\n",
        "      [ 777777, 843400],\n",
        "      [ 8888888, 9222200],\n",
        "      [ 7777777, 8434000],\n",
        "      [ 88888888, 92222000],\n",
        "      [ 77777777, 84340000]])\n",
        "\n",
        "\n",
        "def make_addition_questions():\n",
        "  _, _, s0 = make_s0_questions()\n",
        "  _, _, s1 = make_s1_questions()\n",
        "  _, _, s2 = make_s2_questions()\n",
        "  _, _, s3 = make_s3_questions()\n",
        "  _, _, s4 = make_s4_questions()\n",
        "  _, _, s5 = make_s5_questions()\n",
        "  _, _, s6 = make_sn_questions()\n",
        "\n",
        "  return addition_major_tag, \"S*\", torch.vstack((s0.cuda(), s1.cuda(), s2.cuda(), s3.cuda(), s4.cuda(), s5.cuda(), s6.cuda()))\n",
        "\n",
        "\n",
        "def make_subtraction_questions():\n",
        "  _, _, m0 = make_m0_questions()\n",
        "  _, _, m1 = make_m1_questions()\n",
        "  _, _, m2 = make_m2_questions()\n",
        "  _, _, m3 = make_m3_questions()\n",
        "  _, _, m4 = make_ng_questions()\n",
        "\n",
        "  return subtraction_major_tag, \"M*\", torch.vstack((m0.cuda(), m1.cuda(), m2.cuda(), m3.cuda(), m4.cuda()))\n",
        "\n",
        "\n",
        "v0 = next(ds) # Could be Add, Sub or Mult\n",
        "v1 = next(ds) # Could be Add, Sub or Mult\n",
        "if cfg.perc_add() > 0 and cfg.perc_sub > 0 :\n",
        "  v0 = data_generator_core( PLUS_INDEX )\n",
        "  v1 = data_generator_core( MINUS_INDEX )\n",
        "\n",
        "\n",
        "# Returns ~1000 random and up to ~150 manually-chosen questions\n",
        "def make_varied_questions():\n",
        "  if cfg.perc_mult == 100 :\n",
        "    return varied_major_tag, \"All\", torch.vstack((v0.cuda(), v1.cuda()))\n",
        "\n",
        "  _, _, s0 = make_s0_questions()\n",
        "  _, _, s1 = make_s1_questions()\n",
        "  _, _, s2 = make_s2_questions()\n",
        "  _, _, s3 = make_s3_questions()\n",
        "  _, _, s4 = make_s4_questions()\n",
        "  _, _, s5 = make_s5_questions()\n",
        "  _, _, s6 = make_sn_questions()\n",
        "\n",
        "  _, _, m0 = make_m0_questions()\n",
        "  _, _, m1 = make_m1_questions()\n",
        "  _, _, m2 = make_m2_questions()\n",
        "  _, _, m3 = make_m3_questions()\n",
        "  _, _, m4 = make_ng_questions()\n",
        "\n",
        "  if cfg.perc_add() == 100 :\n",
        "    return addition_major_tag, \"S*\", torch.vstack((v0.cuda(), s0.cuda(), s1.cuda(), s2.cuda(), s3.cuda(), s4.cuda(), v1.cuda()))\n",
        "\n",
        "  if cfg.perc_sub == 100 :\n",
        "    return subtraction_major_tag, \"M*\", torch.vstack((v0.cuda(), m0.cuda(), m1.cuda(), m2.cuda(), m3.cuda(), m4.cuda(), v1.cuda()))\n",
        "\n",
        "  return varied_major_tag, \"All\", torch.vstack((v0.cuda(), s0.cuda(), m0.cuda(), s1.cuda(), m1.cuda(), s2.cuda(), m2.cuda(), s3.cuda(), m3.cuda(), s4.cuda(), m4.cuda(), s5.cuda(), s6.cuda(), v1.cuda()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyK7QeUjLLFm"
      },
      "source": [
        "# Part 7: Set Up: Map Questions to Quanta\n",
        "\n",
        "Functions to evaluate the \"question complexity\" of questions, and to evlaute the \"answer impact\" quanta of questions with answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHQpt2iOqffb"
      },
      "outputs": [],
      "source": [
        "# Convert \"12345\" to 12345\n",
        "def tokens_to_unsigned_int( q, offset, digits ):\n",
        "  a = 0\n",
        "  for j in range(digits):\n",
        "    a = a * 10 + q[offset+j]\n",
        "  return a\n",
        "\n",
        "\n",
        "# Convert \"-12345\" to -12345\n",
        "def tokens_to_signed_int( q, offset, digits ):\n",
        "  a = tokens_to_unsigned_int( q, offset+1, digits )\n",
        "  if q[offset] == MINUS_INDEX:\n",
        "    a = - a\n",
        "  return a\n",
        "\n",
        "\n",
        "# Convert \"-12345\" to -12345\n",
        "def tokens_to_answer(q):\n",
        "  # offset of sign character\n",
        "  sign_offset = cfg.n_digits*2 + 2\n",
        "\n",
        "  # 5 digit addition yields a 6 digit answer. So cfg.n_digits+1 DIGITS\n",
        "  answer_digits = cfg.n_digits+1\n",
        "\n",
        "  a = tokens_to_unsigned_int( q, sign_offset+1, answer_digits )\n",
        "  if q[sign_offset] == MINUS_INDEX:\n",
        "    a = - a\n",
        "\n",
        "  return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZXEfGBMLPFW"
      },
      "outputs": [],
      "source": [
        "# Compare each digit in the answer. Returns a A645 pattern where '4' means a failed 4th digit. Can fail on the \"-\" sign.\n",
        "def get_answer_impact_str(answer1_str, answer2_str):\n",
        "\n",
        "  impact_str = \"\"\n",
        "  for i in range(cfg.n_digits+2):\n",
        "    impact_str += \"\" if answer2_str[i] == answer1_str[i] else str(cfg.n_digits-i+1)\n",
        "\n",
        "  if impact_str == \"\":\n",
        "    return \"\"\n",
        "\n",
        "  return \"A\" + impact_str\n",
        "\n",
        "\n",
        "# Compare each digit in the answer. Returns a A645 pattern where '4' means a failed 4th digit. Can fail on the \"-\" sign.\n",
        "def get_answer_impact(question_and_answer, answer_str2):\n",
        "\n",
        "  answer1_str = tokens_to_string(question_and_answer[-(cfg.n_digits+2):])\n",
        "\n",
        "  return get_answer_impact_str( answer1_str, answer_str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xiOHRfGKW-W"
      },
      "outputs": [],
      "source": [
        "# Analyse and return the complexity quanta for the Addition (S0 to S4+) or Subtraction (M0 to NG) questions\n",
        "def get_question_complexity(question):\n",
        "  qlist = utils.to_numpy(question)\n",
        "  inputs = qlist[:2*cfg.n_digits+2]\n",
        "  operator = qlist[cfg.n_digits]\n",
        "\n",
        "  if operator == PLUS_INDEX:\n",
        "\n",
        "    # Locate the MC and MS digits (if any)\n",
        "    mc = torch.zeros(cfg.n_digits).to(torch.int64)\n",
        "    ms = torch.zeros(cfg.n_digits).to(torch.int64)\n",
        "    for dn in range(cfg.n_digits):\n",
        "      if inputs[dn] + inputs[dn + cfg.n_digits + 1] == 9:\n",
        "        ms[cfg.n_digits-1-dn] = 1\n",
        "      if inputs[dn] + inputs[dn + cfg.n_digits +1] > 9:\n",
        "        mc[cfg.n_digits-1-dn] = 1\n",
        "\n",
        "    if torch.sum(mc) == 0:\n",
        "      return addition_major_tag, \"S0\"\n",
        "\n",
        "    if torch.sum(ms) == 0:\n",
        "      return addition_major_tag, \"S1\"\n",
        "\n",
        "    for dn in range(cfg.n_digits-4):\n",
        "      if mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1 and ms[dn+3] == 1 and ms[dn+4] == 1:\n",
        "        return addition_major_tag, \"S5\" # MC cascades 4 or more digits\n",
        "\n",
        "    for dn in range(cfg.n_digits-3):\n",
        "      if mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1 and ms[dn+3] == 1:\n",
        "        return addition_major_tag, \"S4\" # MC cascades 3 or more digits\n",
        "\n",
        "    for dn in range(cfg.n_digits-2):\n",
        "      if mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1:\n",
        "        return addition_major_tag, \"S3\" # MC cascades 2 or more digits\n",
        "\n",
        "    for dn in range(cfg.n_digits-1):\n",
        "      if mc[dn] == 1 and ms[dn+1] == 1:\n",
        "        return addition_major_tag, \"S2\" # Simple US 9\n",
        "\n",
        "    return addition_major_tag, \"S1\"\n",
        "\n",
        "\n",
        "  if operator == MINUS_INDEX:\n",
        "    a = tokens_to_unsigned_int( question, 0, cfg.n_digits )\n",
        "    b = tokens_to_unsigned_int( question, cfg.n_digits + 1, cfg.n_digits )\n",
        "    if a - b < 0:\n",
        "      return subtraction_major_tag, \"NG\"\n",
        "\n",
        "    # Locate the B1 and MZ digits (if any)\n",
        "    b1 = torch.zeros(cfg.n_digits).to(torch.int64)\n",
        "    mz = torch.zeros(cfg.n_digits).to(torch.int64)\n",
        "    for dn in range(cfg.n_digits):\n",
        "      if inputs[dn] - inputs[dn + cfg.n_digits + 1] < 0:\n",
        "        b1[cfg.n_digits-1-dn] = 1\n",
        "      if inputs[dn] - inputs[dn + cfg.n_digits +1] == 0:\n",
        "        mz[cfg.n_digits-1-dn] = 1\n",
        "\n",
        "    # Evaluate BaseSub questions - when no column generates a Borrow One\n",
        "    if torch.sum(b1) == 0:\n",
        "      return subtraction_major_tag, \"M0\"\n",
        "\n",
        "    # Evaluate subtraction \"cascade multiple steps\" questions\n",
        "    for dn in range(cfg.n_digits-3):\n",
        "      if b1[dn] == 1 and mz[dn+1] == 1 and mz[dn+2] == 1 and mz[dn+3] == 1:\n",
        "        return subtraction_major_tag, \"M4+\" # B1 cascades 3 or more digits\n",
        "\n",
        "    # Evaluate subtraction \"cascade multiple steps\" questions\n",
        "    for dn in range(cfg.n_digits-2):\n",
        "      if b1[dn] == 1 and mz[dn+1] == 1 and mz[dn+2] == 1:\n",
        "        return subtraction_major_tag, \"M3\" # B1 cascades 2 or more digits\n",
        "\n",
        "    # Evaluate subtraction \"cascade 1\" questions\n",
        "    for dn in range(cfg.n_digits-1):\n",
        "      if b1[dn] == 1 and mz[dn+1] == 1:\n",
        "        return subtraction_major_tag, \"M2\" # B1 cascades 1 digit\n",
        "\n",
        "    return subtraction_major_tag, \"M1\"\n",
        "\n",
        "\n",
        "  if operator == MULT_INDEX:\n",
        "    return multiplication_major_tag, \"MUL\"\n",
        "\n",
        "  # Should never get here\n",
        "  print(\"get_question_complexity OP? exception\", question)\n",
        "  return varied_major_tag, \"OP?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg1z2-YdkWHO"
      },
      "outputs": [],
      "source": [
        "def unit_test_quanta_core(make_questions):\n",
        "  correct_major_tag, correct_complexity, questions = make_questions()\n",
        "  num_questions = questions.shape[0]\n",
        "  print( correct_major_tag+'.'+correct_complexity, \"#Questions=\", num_questions)\n",
        "\n",
        "  for i in range(num_questions):\n",
        "    major_tag, complexity = get_question_complexity(questions[i])\n",
        "    if major_tag != correct_major_tag or complexity != correct_complexity:\n",
        "      print( \"Complexity mismatch:\", correct_major_tag, major_tag, correct_complexity, complexity, questions[i])\n",
        "\n",
        "\n",
        "# Test that our \"sample questions by quanta\" and \"question quanta evaluation\" are aligned.\n",
        "# If this fails, either the sample questions or the evaluation is buggy.\n",
        "def unit_test_quanta():\n",
        "  unit_test_quanta_core(make_s0_questions)\n",
        "  unit_test_quanta_core(make_s1_questions)\n",
        "  unit_test_quanta_core(make_s2_questions)\n",
        "  unit_test_quanta_core(make_s3_questions)\n",
        "  unit_test_quanta_core(make_s4_questions)\n",
        "  unit_test_quanta_core(make_s5_questions)\n",
        "\n",
        "  unit_test_quanta_core(make_m0_questions)\n",
        "  unit_test_quanta_core(make_m1_questions)\n",
        "  unit_test_quanta_core(make_m2_questions)\n",
        "  unit_test_quanta_core(make_m3_questions)\n",
        "  unit_test_quanta_core(make_ng_questions)\n",
        "\n",
        "\n",
        "unit_test_quanta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FwJW0tv4Nf"
      },
      "source": [
        "# Part 8A: Set Up: Question prediction function\n",
        "\n",
        "Create sets of sample questions exercising different quanta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RHrsjKqKhR4"
      },
      "outputs": [],
      "source": [
        "# Build a test batch of random and manually-chosen questions\n",
        "_, _, varied_questions = make_varied_questions();\n",
        "\n",
        "\n",
        "# Run the sample batch, gather the cache\n",
        "main_model.reset_hooks()\n",
        "main_model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = main_model.run_with_cache(varied_questions.cuda())\n",
        "print(sample_cache) # Gives names of datasets in the cache\n",
        "sample_losses_raw, sample_max_prob_tokens = logits_to_tokens_loss(sample_logits, varied_questions.cuda())\n",
        "sample_loss_mean = utils.to_numpy(loss_fn(sample_losses_raw).mean())\n",
        "print(\"Sample Mean Loss\", sample_loss_mean) # Loss < 0.04 is good\n",
        "\n",
        "\n",
        "# attn.hook_z is the \"attention head output\" hook point name (at a specified layer)\n",
        "l_attn_hook_z_name = [utils.get_act_name('z', 0, 'a'),utils.get_act_name('z', 1, 'a'),utils.get_act_name('z', 2, 'a'),utils.get_act_name('z', 3, 'a')] # 'blocks.0.attn.hook_z' etc\n",
        "sample_attn_z = sample_cache[l_attn_hook_z_name[0]]\n",
        "print(\"Sample\", l_attn_hook_z_name[0], sample_attn_z.shape) # gives [350, 22, 3, 170] = num_questions, cfg.n_ctx, n_heads, d_head\n",
        "mean_attn_z = torch.mean(sample_attn_z, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_attn_hook_z_name[0], mean_attn_z.shape) # gives [1, 22, 3, 170] = 1, cfg.n_ctx, n_heads, d_head\n",
        "\n",
        "\n",
        "# hook_resid_pre is the \"pre residual memory update\" hook point name (at a specified layer)\n",
        "l_hook_resid_pre_name = ['blocks.0.hook_resid_pre','blocks.1.hook_resid_pre','blocks.2.hook_resid_pre','blocks.3.hook_resid_pre']\n",
        "\n",
        "\n",
        "# hook_resid_post is the \"post residual memory update\" hook point name (at a specified layer)\n",
        "l_hook_resid_post_name = ['blocks.0.hook_resid_post','blocks.1.hook_resid_post','blocks.2.hook_resid_post','blocks.3.hook_resid_post']\n",
        "sample_resid_post = sample_cache[l_hook_resid_post_name[0]]\n",
        "print(\"Sample\", l_hook_resid_post_name[0], sample_resid_post.shape) # gives [350, 22, 510] = num_questions, cfg.n_ctx, d_model\n",
        "mean_resid_post = torch.mean(sample_resid_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_hook_resid_post_name[0], mean_resid_post.shape) # gives [1, 22, 510] = 1, cfg.n_ctx, d_model\n",
        "\n",
        "\n",
        "# mlp.hook_post is the \"MLP layer\" hook point name (at a specified layer)\n",
        "l_mlp_hook_post_name = [utils.get_act_name('post', 0),utils.get_act_name('post', 1),utils.get_act_name('post', 2),utils.get_act_name('post', 3)] # 'blocks.0.mlp.hook_post' etc\n",
        "sample_mlp_hook_post = sample_cache[l_mlp_hook_post_name[0]]\n",
        "print(\"Sample\", l_mlp_hook_post_name[0], sample_mlp_hook_post.shape) # gives [350, 22, 2040] = num_questions, cfg.n_ctx, cfg.d_mlp\n",
        "mean_mlp_hook_post = torch.mean(sample_mlp_hook_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_mlp_hook_post_name[0], mean_mlp_hook_post.shape) # gives [1, 22, 2040] = 1, cfg.n_ctx, cfg.d_mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjhfLpSW9Jsq"
      },
      "outputs": [],
      "source": [
        "verbose = True\n",
        "\n",
        "class T_Config():\n",
        "  num_questions : int\n",
        "  correct_answers : int\n",
        "  total_mean_loss : float\n",
        "\n",
        "  sum_num_questions : int\n",
        "  sum_correct_answers : int\n",
        "\n",
        "  output = PrettyTable()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.num_questions = 0\n",
        "    self.correct_answers = 0\n",
        "    self.total_mean_loss = 0.0\n",
        "    self.sum_num_questions = 0\n",
        "    self.sum_correct_answers = 0\n",
        "\n",
        "    self.output = PrettyTable()\n",
        "    self.output.field_names = [\"Complexity\", \"#Questions\", \"#Correct\", \"%Correct\", \"Mean loss\"]\n",
        "\n",
        "\n",
        "  # Clear the question summary results\n",
        "  def clear_questions_results(self, title):\n",
        "    global verbose\n",
        "\n",
        "    self.num_questions = 0\n",
        "    self.correct_answers = 0\n",
        "    self.total_mean_loss = 0\n",
        "\n",
        "    if verbose:\n",
        "      print(title)\n",
        "\n",
        "\n",
        "  # Print the question summary results\n",
        "  def print_questions_results(self, prefix):\n",
        "    self.output.add_row([prefix, self.num_questions, str(self.correct_answers), 100*self.correct_answers/self.num_questions, self.total_mean_loss/self.num_questions])\n",
        "    self.sum_num_questions += self.num_questions\n",
        "    self.sum_correct_answers += self.correct_answers\n",
        "\n",
        "\n",
        "  # Print the overall summary results\n",
        "  def print_overall_results(self):\n",
        "    self.output.add_row([\"OVERALL\", self.sum_num_questions, self.sum_correct_answers, \"\", \"\"])\n",
        "    print(self.output.get_formatted_string(out_format=cfg.table_out_format))\n",
        "\n",
        "\n",
        "  # Evidence (not proof) the model is accurate\n",
        "  def might_be_fully_accurate(self):\n",
        "    return self.sum_num_questions == self.sum_correct_answers\n",
        "\n",
        "\n",
        "tcfg = T_Config()\n",
        "tcfg.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56siA_QKe0W"
      },
      "outputs": [],
      "source": [
        "# Ask model to predict answer for each question & collect results\n",
        "def do_questions(questions, show_failures = False):\n",
        "  global verbose\n",
        "\n",
        "  tcfg.num_questions = questions.shape[0]\n",
        "\n",
        "  # Run with no hook\n",
        "  all_logits = main_model(questions.cuda())\n",
        "  all_losses_raw, all_max_prob_tokens = logits_to_tokens_loss(all_logits, questions.cuda())\n",
        "\n",
        "  for question_num in range(tcfg.num_questions):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    losses = loss_fn(all_losses_raw[question_num])\n",
        "    mean_loss = utils.to_numpy(losses.mean())\n",
        "    tcfg.total_mean_loss += mean_loss\n",
        "\n",
        "    model_answer_str = tokens_to_string(all_max_prob_tokens[question_num])\n",
        "    model_answer_num = int(model_answer_str)\n",
        "\n",
        "    a = tokens_to_answer(q)\n",
        "\n",
        "    correct = (model_answer_num == a)\n",
        "    if correct :\n",
        "      tcfg.correct_answers += 1\n",
        "\n",
        "    if verbose or (show_failures and not correct):\n",
        "      print(tokens_to_string(q), \"ModelAnswer:\", model_answer_str, \"Loss:\", mean_loss )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN3mEnSRF0DP"
      },
      "outputs": [],
      "source": [
        "def print_question_results( make_questions, show_failures = False):\n",
        "  major_tag, quanta_case, questions = make_questions()\n",
        "  title = major_tag + '.' + quanta_case\n",
        "\n",
        "  tcfg.clear_questions_results(title)\n",
        "  do_questions(questions, show_failures)\n",
        "  tcfg.print_questions_results(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lag8C3d_KkeQ"
      },
      "source": [
        "# Part 8B: Results: Prediction Analysis By Question Complexity\n",
        "This section runs hand-curated test cases to indicate which complexity quanta the model can (probably) handle. Not proof - our test cases might be inadequate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z6RdolRKnnR"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "\n",
        "if cfg.perc_add() > 0:\n",
        "  tcfg.reset()\n",
        "  print_question_results(make_s0_questions)\n",
        "  print_question_results(make_s1_questions)\n",
        "  print_question_results(make_s2_questions)\n",
        "  print_question_results(make_s3_questions)\n",
        "  print_question_results(make_s4_questions)\n",
        "  print_question_results(make_s5_questions)\n",
        "  print_question_results(make_sn_questions)\n",
        "  tcfg.print_overall_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW2SNwxJ-3Kn"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "\n",
        "if cfg.perc_sub > 0:\n",
        "  tcfg.reset()\n",
        "  print_question_results(make_m0_questions)\n",
        "  print_question_results(make_m1_questions)\n",
        "  print_question_results(make_m2_questions)\n",
        "  print_question_results(make_m3_questions)\n",
        "  print_question_results(make_ng_questions)\n",
        "  tcfg.print_overall_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpNvthy48TU0"
      },
      "outputs": [],
      "source": [
        "# Varied questions includes 2 random batches of questions. Show any questions that we can't calculate correctly.\n",
        "tcfg.reset()\n",
        "print_question_results(make_varied_questions, True)\n",
        "tcfg.print_overall_results()\n",
        "\n",
        "model_might_be_fully_accurate = tcfg.might_be_fully_accurate()\n",
        "if not model_might_be_fully_accurate:\n",
        "  print()\n",
        "  print(\"WARNING: Model is not fully accurate. It failed some test questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw4jCYh-lTfW"
      },
      "source": [
        "#Part 10: Set Up: Quanta results by token position\n",
        "Define tools to count failures by quanta / metrics. Use prefix \"q_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqpvFgN6zAZr"
      },
      "outputs": [],
      "source": [
        "def increment_dictionary_case_count(dictionary, the_case):\n",
        "  if the_case in dictionary:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    dictionary[the_case] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    dictionary[the_case] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzISTWrxMNbp"
      },
      "outputs": [],
      "source": [
        "class Q_Config():\n",
        "  # Build up a count of questions by complexity (e.g. Add.S0, Add.S1, Sub.M0, Sub.NG, etc)\n",
        "  question_complexity_counts = {}\n",
        "  # Build up a list of question failure by complexity (e.g. Add.S0, Add.S1, Sub.M0, Sub.NG, etc)\n",
        "  question_complexity_fails = {}\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.question_complexity_counts = {}\n",
        "    self.question_complexity_fails = {}\n",
        "\n",
        "\n",
        "  def add_questions_complexity_count(self, questions):\n",
        "    for i in range(questions.shape[0]):\n",
        "      major_tag, the_complexity = get_question_complexity(questions[i])\n",
        "      increment_dictionary_case_count(self.question_complexity_counts, the_complexity)\n",
        "\n",
        "\n",
        "  def add_question_complexity_fail(self, the_complexity):\n",
        "    increment_dictionary_case_count(self.question_complexity_fails, the_complexity)\n",
        "\n",
        "\n",
        "qcfg = Q_Config()\n",
        "qcfg.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TudM4_8PLTh1"
      },
      "outputs": [],
      "source": [
        "def q_total_complexity_fails():\n",
        "  answer = 0\n",
        "  for _, value in qcfg.question_complexity_fails.items():\n",
        "    answer += value\n",
        "  return answer\n",
        "\n",
        "\n",
        "def q_get_complexity_fails():\n",
        "  results = \"\"\n",
        "\n",
        "  if len(qcfg.question_complexity_fails) > 0:\n",
        "    sorted_fails = dict(sorted(qcfg.question_complexity_fails.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    for key, value in sorted_fails.items():\n",
        "      percent = round(100 * value / qcfg.question_complexity_counts[key])\n",
        "      results += \"%\" + key + \"=\" + str(percent)+ \" \"\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXBYdxj-jLZc"
      },
      "source": [
        "# Part 12A: Set Up: Predict Questions and Evaluate Quanta\n",
        "\n",
        "Get model to predict given question answers, with an ablation hook, and categorise which questions fail by quanta / metrics. Use prefix \"q_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03wT2QJL34aP"
      },
      "outputs": [],
      "source": [
        "def q_predict_questions(questions, the_hook):\n",
        "\n",
        "  main_model.reset_hooks()\n",
        "  main_model.set_use_attn_result(True)\n",
        "\n",
        "  all_logits = main_model.run_with_hooks(questions.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "  all_losses_raw, all_max_prob_tokens = logits_to_tokens_loss(all_logits, questions.cuda())\n",
        "\n",
        "  qcfg.reset()\n",
        "  qcfg.add_questions_complexity_count(questions)\n",
        "\n",
        "  for question_num in range(questions.shape[0]):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    the_loss_mean = utils.to_numpy(loss_fn(all_losses_raw[question_num]).mean())\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated token position)\n",
        "    if the_loss_mean > ccfg.threshold:\n",
        "      answer_str = tokens_to_string(all_max_prob_tokens[question_num])\n",
        "\n",
        "      # Only count the question if the model got the question wrong\n",
        "      impact_str = get_answer_impact( q, answer_str )\n",
        "      if 'A' in impact_str:\n",
        "        major_tag, the_complexity = get_question_complexity(q)\n",
        "\n",
        "        qcfg.add_question_complexity_fail(the_complexity)\n",
        "\n",
        "        if verbose :\n",
        "          print(tokens_to_string(q), \"Q: ModelAnswer:\", answer_str, \"Complexity:\", the_complexity, \"Impact:\", impact_str, \"Loss:\", the_loss_mean )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-59uVTC2k6Pz"
      },
      "outputs": [],
      "source": [
        "# Row in the quanta map (covering L0H0 .. L0Hn L0MLP L1H0 .. L1Hn L1MLP ...):\n",
        "def quanta_row(the_layer, the_head):\n",
        "  return the_layer * (cfg.n_heads+1) + the_head\n",
        "\n",
        "\n",
        "# Number of rows in quanta map (covering L0H0 .. L0Hn L0MLP L1H0 .. L1Hn L1MLP ...):\n",
        "def quanta_rows():\n",
        "  return (cfg.n_heads + 1) * cfg.n_layers\n",
        "\n",
        "\n",
        "def get_quanta_row_heading(r):\n",
        "  head = r % (cfg.n_heads + 1)\n",
        "  layer = r // (cfg.n_heads + 1)\n",
        "  return \"L\" + str(layer) + (\"H\" + str(head) if head < cfg.n_heads else \"MLP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx7LwECBLajQ"
      },
      "outputs": [],
      "source": [
        "class C_Config():\n",
        "  output = PrettyTable()\n",
        "  threshold : int = 0.01\n",
        "\n",
        "  useful_positions = []  # sparce ordered list of useful token positions e.g. 0,1,8,9,10,11\n",
        "  useful_rows = []  # sparce ordered list of useful quanta_rows e.g. 0,1,2,3,4,7\n",
        "\n",
        "  curr_position : int = 0   # zero-based token position to ablate\n",
        "\n",
        "\n",
        "  # Add a token position that we know is used in calculations\n",
        "  def add_useful_position(self, position):\n",
        "    if not (position in self.useful_positions):\n",
        "      self.useful_positions += [position]\n",
        "\n",
        "\n",
        "  # Add a quanta row that we know is used in calculations\n",
        "  def add_useful_row(self, row):\n",
        "    if not (row in self.useful_rows):\n",
        "      self.useful_rows += [row]\n",
        "\n",
        "\n",
        "ccfg = C_Config()\n",
        "ccfg.output.field_names = [\"Position\", \"Fails\", \"% Fails by Complexity\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGL57MRBLdUh"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "\n",
        "\n",
        "def c_set_resid_post_hook(value, hook):\n",
        "  global ccfg\n",
        "\n",
        "  #print( \"In hook\", l_hook_resid_post_name[ccfg.layer], ccfg.ablate, ccfg.curr_position, value.shape) # Get [64, 22, 510] = cfg.batch_size, num_tokens, d_model\n",
        "\n",
        "  # Copy the mean resid post values in position N to all the batch questions\n",
        "  value[:,ccfg.curr_position,:] = mean_resid_post[0,ccfg.curr_position,:].clone()\n",
        "\n",
        "\n",
        "num_questions = 0\n",
        "if cfg.n_digits >= 5 :\n",
        "  c_fwd_hooks = [(l_hook_resid_post_name[0], c_set_resid_post_hook)] if cfg.n_layers == 1 else [(l_hook_resid_post_name[0], c_set_resid_post_hook),(l_hook_resid_post_name[1], c_set_resid_post_hook)]\n",
        "\n",
        "  num_questions = varied_questions.shape[0]\n",
        "\n",
        "  for ccfg.curr_position in range(cfg.n_ctx):\n",
        "    q_predict_questions(varied_questions, c_fwd_hooks)\n",
        "\n",
        "    num_fails = q_total_complexity_fails()\n",
        "    perc_fails = 0\n",
        "    if num_fails > 0:\n",
        "      perc_fails = round(100 * num_fails / num_questions)\n",
        "\n",
        "      ccfg.add_useful_position(ccfg.curr_position)\n",
        "\n",
        "    ccfg.output.add_row([str(ccfg.curr_position), num_fails, q_get_complexity_fails()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmsGWUbILYin"
      },
      "source": [
        "# Part 12B: Results: Ablate ALL Heads in EACH token position. What is the impact?\n",
        "\n",
        "Here we ablate all heads in each token position (overriding the model memory aka residual stream) and see if loss increases. If loss increases the token position is used by the algorithm. Unused token positions can be excluded from further analysis. Use \"C_\" prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpRp5YMmLe1y"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "print(\"num_questions=\", num_questions, \"min_useful_position=\", min(ccfg.useful_positions), \"max_useful_position=\", max(ccfg.useful_positions) )\n",
        "print()\n",
        "print(ccfg.output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904WBkTOLg_5"
      },
      "source": [
        "# Part 14: Setup: Quanta results by useful node (aka cell)\n",
        "\n",
        "Uses \"u_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2gWgm-2J0Lo"
      },
      "outputs": [],
      "source": [
        "class UsefulCell():\n",
        "  # Position.Layer.Head of the cell\n",
        "  position: int  # token-position. Zero to cfg.n_ctx - 1\n",
        "  layer: int\n",
        "  head: int\n",
        "\n",
        "  # Tags related to the cell of form \"MajorVersion.MinorVersion\"\n",
        "  tags: list\n",
        "\n",
        "\n",
        "  # Is this cell an attention head? If not, it must be an MLP layer\n",
        "  def is_head(self):\n",
        "    return self.head != cfg.n_heads\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.position = -1\n",
        "    self.layer = -1\n",
        "    self.head = -1\n",
        "    self.tags = []\n",
        "\n",
        "\n",
        "  # Remove some/all tafs from this cell\n",
        "  def reset_tags(self, major_version):\n",
        "    if major_version == \"\":\n",
        "      self.tags = []\n",
        "    else:\n",
        "      self.tags = [s for s in self.tags if not s.startswith(major_version)]\n",
        "\n",
        "\n",
        "  # Row in a table that this cell is drawn\n",
        "  def cell_row(self):\n",
        "    return quanta_row(self.layer, self.head)\n",
        "\n",
        "\n",
        "  # Add a tag to this cell (if not already present)\n",
        "  def add_tag(self, tag):\n",
        "    if tag != \"\" and (not (tag in self.tags)):\n",
        "      self.tags += [tag]\n",
        "\n",
        "\n",
        "  # Return tags with the matching major\n",
        "  def filter_tags(self, major_version):\n",
        "    assert major_version != \"\"\n",
        "\n",
        "    # Filter strings that start with the given major version\n",
        "    filtered_strings = [s for s in self.tags if s.startswith(major_version)]\n",
        "\n",
        "    # Extract minor versions\n",
        "    minor_versions = [s.split('.')[1] for s in filtered_strings]\n",
        "\n",
        "    return minor_versions\n",
        "\n",
        "\n",
        "  # Return minimum tag with the matching major and minor versions\n",
        "  def min_tag_suffix(self, major_version, minor_version, show_plus = False):\n",
        "    suffixes = self.filter_tags(major_version)\n",
        "\n",
        "    if minor_version != \"\":\n",
        "      suffixes = [s for s in suffixes if s.startswith(minor_version)]\n",
        "\n",
        "    answer = min(suffixes) if suffixes else \"\"\n",
        "\n",
        "    if show_plus and len(suffixes) > 1:\n",
        "      answer += \"+\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "  # Return the only tag with the matching major_version\n",
        "  def only_tag(self, major_version):\n",
        "    assert major_version != \"\"\n",
        "\n",
        "    filtered_strings = [s for s in self.tags if s.startswith(major_version)]\n",
        "\n",
        "    num_strings = len(filtered_strings)\n",
        "    if num_strings > 1:\n",
        "      print(\"only_tag logic failure\", major_version, num_strings, filtered_strings)\n",
        "      assert False\n",
        "\n",
        "    return filtered_strings[0].split('.')[1] if num_strings == 1 else \"\"\n",
        "\n",
        "\n",
        "  def to_dict(self):\n",
        "    return {\n",
        "      \"position\": self.position,\n",
        "      \"layer\": self.layer,\n",
        "      \"head\": self.head,\n",
        "      \"tags\": self.tags\n",
        "    }\n",
        "\n",
        "\n",
        "  def __init__(self, position, layer, head, tags):\n",
        "    self.position = position\n",
        "    self.layer = layer\n",
        "    self.head = head\n",
        "    self.tags = tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTaSo-UzLlZ_"
      },
      "outputs": [],
      "source": [
        "class U_Config():\n",
        "  useful_cells = []\n",
        "\n",
        "  curr_position : int\n",
        "  curr_layer : int\n",
        "  curr_head : int\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.useful_cells = []\n",
        "    self.curr_position = 0\n",
        "    self.curr_layer = 0\n",
        "    self.curr_head = 0\n",
        "\n",
        "\n",
        "  def reset_tags(self, major_version = \"\"):\n",
        "    for cell in self.useful_cells:\n",
        "      cell.reset_tags(major_version)\n",
        "\n",
        "\n",
        "  def get_cell( self, the_row, the_position ):\n",
        "    for cell in self.useful_cells:\n",
        "      if cell.position == the_position and cell.cell_row() == the_row:\n",
        "        return cell\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def add_cell_tag( self, tag ):\n",
        "    assert self.curr_position  >= 0\n",
        "    assert self.curr_layer >= 0\n",
        "    assert self.curr_head >= 0\n",
        "    assert self.curr_position < cfg.n_ctx\n",
        "    assert self.curr_layer < cfg.n_layers\n",
        "    assert self.curr_head <= cfg.n_heads\n",
        "\n",
        "    the_row = quanta_row(self.curr_layer, self.curr_head)\n",
        "    assert the_row >= 0\n",
        "\n",
        "    the_cell = self.get_cell(the_row, self.curr_position )\n",
        "    if the_cell == None:\n",
        "\n",
        "      the_cell = UsefulCell(self.curr_position , self.curr_layer, self.curr_head, [])\n",
        "\n",
        "      self.useful_cells += [the_cell]\n",
        "\n",
        "    the_cell.add_tag(tag)\n",
        "\n",
        "\n",
        "ucfg = U_Config()\n",
        "ucfg.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZG-kUBMph9D"
      },
      "outputs": [],
      "source": [
        "def u_predict_questions(questions, the_hook):\n",
        "\n",
        "  main_model.reset_hooks()\n",
        "  main_model.set_use_attn_result(True)\n",
        "\n",
        "  all_logits = main_model.run_with_hooks(questions.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "  all_losses_raw, all_max_prob_tokens = logits_to_tokens_loss(all_logits, questions.cuda())\n",
        "\n",
        "  num_fails = 0\n",
        "  for question_num in range(questions.shape[0]):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    the_loss_mean = utils.to_numpy(loss_fn(all_losses_raw[question_num]).mean())\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated token position)\n",
        "    if the_loss_mean > ccfg.threshold:\n",
        "      answer_str = tokens_to_string(all_max_prob_tokens[question_num])\n",
        "\n",
        "      impact_str = get_answer_impact( q, answer_str )\n",
        "      # Only count the question if the model got the question wrong\n",
        "      if 'A' in impact_str:\n",
        "        num_fails += 1\n",
        "        major_tag, the_complexity = get_question_complexity(q)\n",
        "\n",
        "        # Add question complexity quanta\n",
        "        ucfg.add_cell_tag( major_tag + \".\" + the_complexity )\n",
        "\n",
        "        # Add answer digit impact quanta\n",
        "        ucfg.add_cell_tag( impact_major_tag + \".\" + impact_str )\n",
        "\n",
        "        if verbose :\n",
        "          print(tokens_to_string(q), \"U: ModelAnswer:\", answer_str, \"Complexity:\", the_complexity, \"Impact:\", impact_str, \"Loss:\", the_loss_mean )\n",
        "\n",
        "  if num_fails > 0:\n",
        "    # Add percentage failure quanta\n",
        "    perc = int( 100.0 * num_fails / len(questions))\n",
        "    perc_tag = perc_major_tag + '.' + str(perc)\n",
        "    ucfg.add_cell_tag( perc_tag)\n",
        "\n",
        "    ccfg.add_useful_row(quanta_row(ucfg.curr_layer, ucfg.curr_head))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOxNc9SAMGRH"
      },
      "outputs": [],
      "source": [
        "def u_mlp_hook_post(value, hook):\n",
        "  #print( \"In u_mlp_hook_post\", value.shape) # Get [1, 22, 2040] = ???, cfg.n_ctx, cfg.d_mlp\n",
        "\n",
        "  # Mean ablate. Copy the mean resid post values in position N to the MLP\n",
        "  value[:,ucfg.curr_position,:] =  mean_mlp_hook_post[:,ucfg.curr_position,:].clone()\n",
        "\n",
        "\n",
        "# Ablating the MLP in each layer in each position and seeing if the loss increases shows which head+layer+MLP are used by the algorithm.\n",
        "def u_mlp_perform_all(questions):\n",
        "  ucfg.curr_head = cfg.n_heads\n",
        "  for ucfg.curr_position in ccfg.useful_positions:\n",
        "    for ucfg.curr_layer in range(cfg.n_layers):\n",
        "      the_hook = [(l_mlp_hook_post_name[ucfg.curr_layer], u_mlp_hook_post)]\n",
        "      u_predict_questions(questions, the_hook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2EnisO6MMGQ"
      },
      "outputs": [],
      "source": [
        "def u_head_attn_hook_z(value, hook):\n",
        "  # print( \"In u_head_attn_hook_z\", value.shape) # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "\n",
        "  # Mean ablate. Copy the mean resid post values in position N to all the batch questions\n",
        "  value[:,ucfg.curr_position,ucfg.curr_head,:] = mean_attn_z[:,ucfg.curr_position,ucfg.curr_head,:].clone()\n",
        "\n",
        "\n",
        "# Ablating each head in each layer in each position and seeing if the loss increases shows which position+layer+head are used by the algorithm.\n",
        "def u_head_perform_all(questions):\n",
        "  for ucfg.curr_position in ccfg.useful_positions:\n",
        "    for ucfg.curr_layer in range(cfg.n_layers):\n",
        "      for ucfg.curr_head in range(cfg.n_heads):\n",
        "        the_hook = [(l_attn_hook_z_name[ucfg.curr_layer], u_head_attn_hook_z)]\n",
        "        u_predict_questions(questions, the_hook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh3f8Jiahdvq"
      },
      "outputs": [],
      "source": [
        "def h_null_attn_z_hook(value, hook):\n",
        "  global ucfg\n",
        "\n",
        "  #print(\"In h_null_attn_z_hook\", value.shape)  # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "\n",
        "\n",
        "def u_calculate_attention_tags(questions):\n",
        "  ucfg.reset_tags(attention_major_tag)\n",
        "\n",
        "  logits, cache = main_model.run_with_cache(questions)\n",
        "\n",
        "  all_attention_weights = []\n",
        "  for layer in range(cfg.n_layers):\n",
        "    attention_weights = cache[\"pattern\", layer, \"attn\"]\n",
        "    #print(attention_weights.shape) # 512, 4, 22, 22 = cfg.batch_size, cfg.n_heads, cfg.n_ctx, cfg.n_ctx\n",
        "\n",
        "    average_attention_weights = attention_weights.mean(dim=0)\n",
        "    #print(average_attention_weights.shape) # 4, 22, 22 = cfg.n_heads, cfg.n_ctx, cfg.n_ctx\n",
        "\n",
        "    all_attention_weights += [average_attention_weights]\n",
        "\n",
        "\n",
        "  for cell in ucfg.useful_cells:\n",
        "    if cell.is_head():\n",
        "      # Get attention weights for this token in this head\n",
        "      layer_weights = all_attention_weights[cell.layer]\n",
        "      weights = layer_weights[cell.head, cell.position, :]\n",
        "\n",
        "      top_tokens = torch.topk(weights, 4)\n",
        "      total_attention = weights.sum()\n",
        "      attention_percentage = top_tokens.values / total_attention * 100\n",
        "\n",
        "      # Add up to 4 tags with percs per head\n",
        "      for idx, token_idx in enumerate(top_tokens.indices):\n",
        "        perc = attention_percentage[idx]\n",
        "        if perc >= 1.0:\n",
        "          ucfg.curr_position = cell.position\n",
        "          ucfg.curr_layer = cell.layer\n",
        "          ucfg.curr_head = cell.head\n",
        "          ucfg.add_cell_tag( f\"{attention_major_tag}.{token_idx}={perc:.0f}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as9ot9RMMTAi"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "ucfg.reset()\n",
        "u_mlp_perform_all(varied_questions)\n",
        "u_head_perform_all(varied_questions)\n",
        "u_calculate_attention_tags(varied_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vJsX2Z1eDVx"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format\n",
        "print( \"Saving useful cell list with quanta tags:\", main_fname_json)\n",
        "\n",
        "dict_list = [cell.to_dict() for cell in ucfg.useful_cells]\n",
        "with open(main_fname_json, 'w') as file:\n",
        "    json.dump(dict_list, file, default=lambda o: o.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmQHiLALp-3"
      },
      "source": [
        " # Part 15: Set up: Quanta 2D map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bqWK2AWiLXp"
      },
      "outputs": [],
      "source": [
        "# Define a colormap for use with graphing\n",
        "def create_custom_colormap():\n",
        "    colors = [\"green\", \"yellow\"]\n",
        "    return mcolors.LinearSegmentedColormap.from_list(\"custom_colormap\", colors)\n",
        "\n",
        "\n",
        "# Blend the color with white to make it paler\n",
        "def pale_color(color, factor=0.5):\n",
        "    color_array = np.array(color)\n",
        "    white = np.array([1, 1, 1, 1])\n",
        "    return white * factor + color_array * (1 - factor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class quanta_result:\n",
        "  model_row : int = 0\n",
        "  model_col : int = 0\n",
        "  cell_text : str = \"\"\n",
        "  color_index :int = -1\n",
        "\n",
        "  def __init__(self, model_row, model_col, cell_text, color_index):\n",
        "    self.model_row = model_row\n",
        "    self.model_col = model_col\n",
        "    self.cell_text = cell_text\n",
        "    self.color_index = color_index\n",
        "\n",
        "\n",
        "def calc_quanta_results( major_version, minor_version, get_cell_details, shades ):\n",
        "\n",
        "  quanta_results = []\n",
        "\n",
        "  for raw_row in ccfg.useful_rows:\n",
        "    for raw_col in ccfg.useful_positions:\n",
        "      cell_text, color_index = get_cell_details(raw_row, raw_col, major_version, minor_version, shades)\n",
        "      if cell_text != \"\" :\n",
        "        quanta_results +=[quanta_result(model_row=raw_row, model_col=raw_col, cell_text=cell_text, color_index=color_index )]\n",
        "\n",
        "  return quanta_results\n",
        "\n",
        "\n",
        "def find_quanta_result_by_row_col(row, col, quanta_results):\n",
        "    for result in quanta_results:\n",
        "        if result.model_row == row and result.model_col == col:\n",
        "            return result\n",
        "    return None"
      ],
      "metadata": {
        "id": "6-h5jZBEMBD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeObQk2kzAv7"
      },
      "outputs": [],
      "source": [
        "# Convert token positions to D5, .., D0, -, D5', .., D0', =, -, A6, .., A0\n",
        "def token_position_to_name( position ):\n",
        "  if position < cfg.n_digits:\n",
        "    return \"D\" + str(cfg.n_digits-position-1)\n",
        "\n",
        "  if position == cfg.n_digits:\n",
        "    return \"*\" # Stands in for operation +, - or *. We use * as it can only be an operation (not part of the answer.)\n",
        "\n",
        "  if position <= 2 * cfg.n_digits:\n",
        "    return \"D'\" + str(2*cfg.n_digits-position)\n",
        "\n",
        "  if position == 2 * cfg.n_digits + 1:\n",
        "    return \"=\"\n",
        "\n",
        "  if position == 2 * cfg.n_digits + 2:\n",
        "    return \"+\"\n",
        "\n",
        "  return \"A\" + str(3*cfg.n_digits-position+3)\n",
        "\n",
        "\n",
        "def unit_test_token_position_to_name():\n",
        "  for i in range (cfg.n_ctx):\n",
        "    print(token_position_to_name(i))\n",
        "\n",
        "\n",
        "# unit_test_token_position_to_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emcB0_CpYTaJ"
      },
      "outputs": [],
      "source": [
        "def show_quanta_add_patch(ax, j, row, cell_color):\n",
        "  ax.add_patch(plt.Rectangle((j, row), 1, 1, fill=True, color=cell_color))\n",
        "\n",
        "\n",
        "def show_quanta_map( title, custom_cmap, shades, major_version, minor_version, get_cell_details, base_fontsize = 10, max_width = 10):\n",
        "\n",
        "  quanta_results = calc_quanta_results(major_version, minor_version, get_cell_details, shades)\n",
        "\n",
        "  distinct_rows = set()\n",
        "  distinct_cols = set()\n",
        "\n",
        "  for result in quanta_results:\n",
        "      distinct_rows.add(result.model_row)\n",
        "      distinct_cols.add(result.model_col)\n",
        "\n",
        "  distinct_rows = sorted(distinct_rows)\n",
        "  distinct_cols = sorted(distinct_cols)\n",
        "\n",
        "  print_config()\n",
        "  print()\n",
        "\n",
        "  # Create figure and axes\n",
        "  fig1, ax1 = plt.subplots(figsize=(2*len(distinct_cols)/3, 2*len(distinct_rows)/3))  # Adjust the figure size as needed\n",
        "\n",
        "  # Ensure cells are square\n",
        "  ax1.set_aspect('equal', adjustable='box')\n",
        "  ax1.yaxis.set_tick_params(labelleft=True, labelright=False)\n",
        "\n",
        "  colors = [pale_color(custom_cmap(i/shades)) for i in range(shades)]\n",
        "  vertical_labels = []\n",
        "  horizontal_labels = []\n",
        "  wrapper = textwrap.TextWrapper(width=max_width)\n",
        "\n",
        "\n",
        "  show_row = len(distinct_rows)-1\n",
        "  for raw_row in distinct_rows:\n",
        "    vertical_labels += [get_quanta_row_heading(raw_row)]\n",
        "\n",
        "    show_col = 0\n",
        "    for raw_col in distinct_cols:\n",
        "      cell_color = 'lightgrey'  # Color for empty cells\n",
        "\n",
        "      if show_row == 0:\n",
        "        horizontal_labels += [token_position_to_name(raw_col)]\n",
        "\n",
        "      result = find_quanta_result_by_row_col(raw_row, raw_col, quanta_results)\n",
        "      if result != None:\n",
        "        cell_color = colors[result.color_index] if result.color_index >= 0 else 'lightgrey'\n",
        "        the_fontsize = base_fontsize if len(result.cell_text) < 4 else base_fontsize-1 if len(result.cell_text) < 5 else base_fontsize-2\n",
        "        wrapped_text = wrapper.fill(text=result.cell_text)\n",
        "        ax1.text(show_col + 0.5, show_row + 0.5, wrapped_text, ha='center', va='center', color='black', fontsize=the_fontsize)\n",
        "\n",
        "      show_quanta_add_patch(ax1, show_col, show_row, cell_color)\n",
        "      show_col += 1\n",
        "\n",
        "    show_row -= 1\n",
        "\n",
        "\n",
        "  # Configure x axis\n",
        "  ax1.set_xlim(0, len(horizontal_labels))\n",
        "  ax1.set_xticks(np.arange(0.5, len(horizontal_labels), 1))\n",
        "  ax1.set_xticklabels(horizontal_labels)\n",
        "  ax1.xaxis.tick_top()\n",
        "  ax1.xaxis.set_label_position('top')\n",
        "  ax1.tick_params(axis='x', length=0)\n",
        "  for label in ax1.get_xticklabels():\n",
        "    label.set_fontsize(9)\n",
        "\n",
        "  # Configure y axis\n",
        "  vertical_labels = vertical_labels[::-1] # Reverse the order\n",
        "  ax1.set_ylim(0, len(vertical_labels))\n",
        "  ax1.set_yticks(np.arange(0.5, len(vertical_labels), 1))\n",
        "  ax1.set_yticklabels(vertical_labels)\n",
        "  ax1.tick_params(axis='y', length=0)\n",
        "  for label in ax1.get_yticklabels():\n",
        "    label.set_horizontalalignment('left')\n",
        "    label.set_position((-0.1, 0))  # Adjust the horizontal position\n",
        "\n",
        "  fulltitle = op_prefix + ': ' + title + ' (d{}_l{}_h{})'.format(cfg.n_digits, cfg.n_layers, cfg.n_heads)\n",
        "\n",
        "  if cfg.save_graph_to_file:\n",
        "    print(\"Saving quanta map:\", fulltitle)\n",
        "    filename = fulltitle.replace( ' ', '_').replace( '-', '_').replace( ':', '_')\n",
        "    plt.savefig(filename+\".pdf\", bbox_inches='tight', pad_inches=0)\n",
        "  else:\n",
        "    ax1.set_title(fulltitle + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "  # Show plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpkyhHRoMOSw"
      },
      "source": [
        "# Part 16A: Results: Show failure percentage quanta map\n",
        "\n",
        "Show the percentage failure rate (incorrect prediction) when individual Attention Heads and MLPs are ablated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRW6tiM8hDOB"
      },
      "outputs": [],
      "source": [
        "for cell in ucfg.useful_cells:\n",
        "  print( \"P\"+str(cell.position)+\"L\"+str(cell.layer)+\"H\"+str(cell.head), cell.tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ28dx5YLs0P"
      },
      "outputs": [],
      "source": [
        "def get_quanta_fail_percs( row, col, major_version, minor_version, shades):\n",
        "  cell_text = \"\"\n",
        "  color_index = 0\n",
        "\n",
        "  cell = ucfg.get_cell( row, col )\n",
        "  if cell != None:\n",
        "    cell_text = cell.only_tag( major_version )\n",
        "    value = int(cell_text) if cell_text != \"\" else 0\n",
        "\n",
        "    if value == 100 and ccfg.num_compressed_cols() > 5:\n",
        "      value = 99 # Avoid overlapping figures in the matrix.\n",
        "    color_index = value // shades\n",
        "    cell_text = (str(value) if value > 0 else \"<1\") + \"%\"\n",
        "\n",
        "  return cell_text, color_index\n",
        "\n",
        "\n",
        "show_quanta_map( varied_major_tag, plt.cm.winter, 10, perc_major_tag, \"\", get_quanta_fail_percs, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCfaCT1Puhz"
      },
      "source": [
        "# Part 16B: Result: Show attention quanta map\n",
        "\n",
        "Show attention quanta of useful cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTGoEWHgvFH6"
      },
      "outputs": [],
      "source": [
        "min_attention_perc = 1 # Only show input tokens with >= 1% of attention\n",
        "\n",
        "\n",
        "# Only maps attention heads, not MLP layers\n",
        "def get_quanta_attention_tag(row, col, major_version, minor_version, shades):\n",
        "  cell_text = \"\"\n",
        "  color_index = 0\n",
        "\n",
        "  if not \"MLP\" in get_quanta_row_heading(row):\n",
        "    cell = ucfg.get_cell( row, col )\n",
        "    if cell != None:\n",
        "      sum_perc = 0\n",
        "      for minor_version in cell.filter_tags( major_version ):\n",
        "        cell_parts = minor_version.split(\"=\")\n",
        "        token_pos = int(cell_parts[0])\n",
        "        the_perc = int(cell_parts[1])\n",
        "        if the_perc >= min_attention_perc:\n",
        "          cell_text += token_position_to_name(token_pos) + \" \"\n",
        "          sum_perc += the_perc\n",
        "\n",
        "      cell_text = cell_text.rstrip(\" \")\n",
        "      color_index = 10 - sum_perc // 10    # Want >90% => Dark-Green, and <10% => Yellow\n",
        "\n",
        "  return cell_text, color_index\n",
        "\n",
        "\n",
        "# Only maps attention heads, not MLP layers\n",
        "show_quanta_map( \"Attention per node\", create_custom_colormap(), 10, attention_major_tag, \"\", get_quanta_attention_tag, 9, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7-99ZxDOrbF"
      },
      "source": [
        "# Part 16C - Show question complexity (S*) quanta map\n",
        "\n",
        "Show the \"minimum\" addition purpose of each useful cell by S0 to S5 quanta.\n",
        "Show the \"minimum\" subtraction purpose of each useful cell by M0 to M5 quanta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tAJl0eYO96W"
      },
      "outputs": [],
      "source": [
        "def get_quanta_min_tag(row, col, major_version, minor_version, shades):\n",
        "  cell_text = \"\"\n",
        "  color_index = 0\n",
        "\n",
        "  cell = ucfg.get_cell( row, col )\n",
        "  if cell != None:\n",
        "    cell_text = cell.min_tag_suffix( major_version, minor_version )\n",
        "\n",
        "    if cell_text != \"\" :\n",
        "      color_index = int(cell_text[1]) if len(cell_text) > 1 and cell_text[1].isdigit() else shades-1\n",
        "\n",
        "  return cell_text, color_index\n",
        "\n",
        "\n",
        "def draw_quanta_min_tags( title, major_version, minor_version, shades):\n",
        "  show_quanta_map( title, create_custom_colormap(), shades, major_version, minor_version, get_quanta_min_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyoErRoCA-pz"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_add() > 0:\n",
        "  draw_quanta_min_tags( \"Addition minimum-quanta per node\", addition_major_tag, \"\", 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZzbjxUBQGE"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  draw_quanta_min_tags( \"Subtraction minimum-quanta per node\", \"Sub\", \"\", 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x48x45-RInZb"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  draw_quanta_min_tags( \"Negative-answer subtraction minimum-quanta per node\", \"Sub\", \"NG\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifmtnLoTCN5"
      },
      "source": [
        "# Part 16D - Show answer impact quanta map\n",
        "\n",
        "Show the purpose of each useful cell by impact on the answer digits A0 to A5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HFvO9nw5kx4"
      },
      "outputs": [],
      "source": [
        "def is_sequential(digits):\n",
        "  return all(digits[i].isdigit() and digits[i+1].isdigit() and int(digits[i]) + 1 == int(digits[i+1]) for i in range(len(digits) - 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkRnSVQywfkr"
      },
      "outputs": [],
      "source": [
        "def remove_duplicate_digits(input_string):\n",
        "    seen = set()\n",
        "    result = \"\"\n",
        "    for char in input_string:\n",
        "        if char not in seen:\n",
        "            seen.add(char)\n",
        "            result += char\n",
        "    return result\n",
        "\n",
        "# Unit test\n",
        "# print( remove_duplicate_digits(\"1231231278321\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K9PyCKYTUzX"
      },
      "outputs": [],
      "source": [
        "def get_impact_quanta_range( row, col, major_version, minor_version, shades):\n",
        "\n",
        "  cell_text = \"\"\n",
        "  color_index = 0\n",
        "\n",
        "  cell = ucfg.get_cell( row, col )\n",
        "  if cell != None and len(cell.tags) > 0:\n",
        "\n",
        "    cell_texts = cell.filter_tags( major_version )\n",
        "    if len(cell_texts) > 0:\n",
        "\n",
        "      # Check for '-' sign\n",
        "      has_dash = any('-' in s for s in cell_texts)\n",
        "\n",
        "      digits = \"\"\n",
        "      for s in cell_texts:\n",
        "        digits += ''.join(filter(str.isdigit, s))\n",
        "      digits = sorted(remove_duplicate_digits(digits))\n",
        "\n",
        "      if len(digits) >= 3 and is_sequential(digits):\n",
        "        digits = f\"{digits[0]}..{digits[-1]}\"\n",
        "\n",
        "      # Joining numbers with the appropriate prefix\n",
        "      cell_text = (\"A-\" if has_dash else \"A\") + ''.join(digits)\n",
        "\n",
        "      color_index = int(cell_text[1]) if len(cell_text) > 1 and cell_text[1].isdigit() else shades-1\n",
        "\n",
        "  return cell_text, color_index\n",
        "\n",
        "\n",
        "show_quanta_map( \"Answer-digit-impact per node\", create_custom_colormap(), cfg.n_digits+2, impact_major_tag, \"\", get_impact_quanta_range, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHSY3blNMe7I"
      },
      "source": [
        "#Part 18: Set Up: Calc and graph PCA decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCiBsiQAMhS_"
      },
      "outputs": [],
      "source": [
        "tn_questions = 100\n",
        "\n",
        "\n",
        "def make_t_questions(test_digit, test_case, operation):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "\n",
        "\n",
        "      if operation == PLUS_INDEX:\n",
        "        if test_case == 8:\n",
        "          # These are n_digit addition questions where the first test_digits add up from 0 to 8\n",
        "          x = random.randint(0, 8)\n",
        "          y = random.randint(0, 8-x)\n",
        "        if test_case == 9:\n",
        "          # These are n_digit addition questions where the first test_digits add up to 9\n",
        "          x = random.randint(0, 9)\n",
        "          y = 9 - x\n",
        "        if test_case == 10:\n",
        "          # These are n_digit addition questions where the first test_digits add up to 10 to 18\n",
        "          x = random.randint(1, 9)\n",
        "          y = random.randint(10-x, 9)\n",
        "\n",
        "\n",
        "      if operation == MINUS_INDEX:\n",
        "        if test_case == 8:\n",
        "          # These are n_digit subtraction questions where the first test_digits difference is negative\n",
        "          x = random.randint(0, 8)\n",
        "          y = random.randint(x+1, 9)\n",
        "        if test_case == 9:\n",
        "          # These are n_digit subtraction questions where the first test_digits difference is zero\n",
        "          x = random.randint(0, 9)\n",
        "          y = x\n",
        "        if test_case == 10:\n",
        "          # These are n_digit subtraction questions where the first test_digits difference is positive\n",
        "          x = random.randint(0, 9)\n",
        "          y = random.randint(0, x-1)\n",
        "\n",
        "\n",
        "      # Randomise the last test_digits-1 digits of both numbers\n",
        "      x = x * limit + random.randint(0, limit-1)\n",
        "      y = y * limit + random.randint(0, limit-1)\n",
        "      questions.append([x, y])\n",
        "    return make_questions(operation, questions)\n",
        "\n",
        "\n",
        "\n",
        "def make_tricase_questions(test_digit, operation):\n",
        "  q1 = make_t_questions(test_digit, 8, operation)\n",
        "  q2 = make_t_questions(test_digit, 9, operation)\n",
        "  q3 = make_t_questions(test_digit, 10, operation)\n",
        "\n",
        "  questions = torch.vstack((q1, q2, q3))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWCPgoQZMjgc"
      },
      "outputs": [],
      "source": [
        "# Do one Principal Component Analysis\n",
        "def calc_tricase_pca(t_position, t_layer, t_head, t_digit, operation):\n",
        "  global tn_questions\n",
        "\n",
        "  t_questions = make_tricase_questions(t_digit, operation)\n",
        "\n",
        "  t_logits, t_cache = main_model.run_with_cache(t_questions)\n",
        "\n",
        "  # Gather attention patterns for all the (randomly chosen) questions\n",
        "  attention_outputs = []\n",
        "  for i in range(len(t_questions)):\n",
        "\n",
        "    # Output of individual heads, without final bias\n",
        "    attention_cache=t_cache[\"result\", t_layer, \"attn\"] # Output of individual heads, without final bias\n",
        "    attention_output=attention_cache[i]  # Shape [n_ctx, n_head, d_model]\n",
        "    attention_outputs.append(attention_output[t_position, t_head, :])\n",
        "\n",
        "  attn_outputs = torch.stack(attention_outputs, dim=0).cpu()\n",
        "\n",
        "  pca = PCA(n_components=6)\n",
        "  pca.fit(attn_outputs)\n",
        "  pca_attn_outputs = pca.transform(attn_outputs)\n",
        "\n",
        "  title = tokens_to_string([operation]) + 'P' + str(t_position) + '.L' + str(t_layer) + '.H'+str(t_head) + ', A'+str(t_digit) + ', EVR[0]=' + str(int(round(pca.explained_variance_ratio_[0]*100,0))) + '%'\n",
        "\n",
        "  return (pca, pca_attn_outputs, title)\n",
        "\n",
        "\n",
        "# Plot one PCA scatter graph\n",
        "def graph_pca(pca, pca_attn_outputs, ax, title):\n",
        "  global tn_questions\n",
        "\n",
        "  ax.scatter(pca_attn_outputs[:tn_questions, 0], pca_attn_outputs[:tn_questions, 1], color='red', label='T8 (0-8)') # t8 questions\n",
        "  ax.scatter(pca_attn_outputs[tn_questions:2*tn_questions, 0], pca_attn_outputs[tn_questions:2*tn_questions, 1], color='green', label='T9') # t9 questions\n",
        "  ax.scatter(pca_attn_outputs[2*tn_questions:, 0], pca_attn_outputs[2*tn_questions:, 1], color='blue', label='T10 (10-18)') # t10 questions\n",
        "\n",
        "  if title != \"\" :\n",
        "    ax.set_title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk2K3y7pMlQr"
      },
      "outputs": [],
      "source": [
        "# Graph the PCA of Pasn.Ln.Hn's attention pattern, using T8, T9, T10 questions that differ in the An digit\n",
        "def add_one_pca_subplot(ax, t_position, t_layer, t_head, t_digit, operation):\n",
        "  try:\n",
        "    pca, pca_attn_outputs, title = calc_tricase_pca(t_position, t_layer, t_head, t_digit, operation)\n",
        "    graph_pca( pca, pca_attn_outputs, ax, title)\n",
        "  except Exception as e:\n",
        "    desc = \"add_one_pca_subplot(\" + str(t_position) + \",\"+ str(t_layer) + \",\"+ str(t_head) + \",\"+ str(t_digit) + \",\"+ str(operation) + \")\"\n",
        "    print( desc + \" Failed:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEF7MQqCMngv"
      },
      "outputs": [],
      "source": [
        "def save_plt_to_file( full_title ):\n",
        "  if cfg.save_graph_to_file:\n",
        "    filename = full_title.replace(\" \", \"_\").replace(\",\", \"\").replace(\":\", \"_\")  + '.pdf'\n",
        "    plt.savefig(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbiau9foMp3h"
      },
      "source": [
        "#Part 19: Results: add_d6_l2_h3_train15K PCA decomposition tri-state results\n",
        "\n",
        "Plot attention heads in the positions 8 to 16 with a clear \"tri-state\" response to (exactly) one An."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ5fS3XNMs8e"
      },
      "outputs": [],
      "source": [
        "if not use_pca:\n",
        "  print( \"PCA library failed to import. So PCA not done\")\n",
        "\n",
        "if use_pca and model_name == \"add_d6_l2_h3_train15K\":\n",
        "  op = PLUS_INDEX\n",
        "\n",
        "  fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "  if model_name == \"add_d5_l2_h3_train15K\" :\n",
        "    fig, axs = plt.subplots(4, 2)\n",
        "    fig.set_figheight(8)\n",
        "    fig.set_figwidth(5)\n",
        "\n",
        "    # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "    add_one_pca_subplot(axs[0, 0], 8, 0, 1, 2, op)    # P8.L0.H1 is interpretable only for A2\n",
        "    add_one_pca_subplot(axs[0, 1], 9, 0, 1, 1, op)    # P9.L0.H1 is interpretable only for A1\n",
        "    add_one_pca_subplot(axs[1, 0], 11, 0, 1, 3, op)   # P11.L0.H1 is interpretable only for A3\n",
        "    add_one_pca_subplot(axs[1, 1], 11, 0, 2, 4, op)   # P11.L0.H2 is interpretable only for A4\n",
        "    add_one_pca_subplot(axs[2, 0], 12, 0, 1, 3, op)   # P12.L0.H1 is interpretable only for A3\n",
        "    add_one_pca_subplot(axs[2, 1], 13, 0, 1, 2, op)   # P13.L0.H1 is interpretable only for A2\n",
        "    add_one_pca_subplot(axs[3, 0], 14, 0, 1, 1, op)   # P14.L0.H1 is interpretable only for A1\n",
        "\n",
        "  if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "    fig, axs = plt.subplots(5, 2)\n",
        "    fig.set_figheight(8)\n",
        "    fig.set_figwidth(5)\n",
        "\n",
        "    # Plot all useful attention heads in the positions 10 to 17 with the clearest An selected\n",
        "    add_one_pca_subplot(axs[0, 0], 11, 0, 0, 2, op)   # P11.L0.H0 is interpretable only for A2\n",
        "    add_one_pca_subplot(axs[0, 1], 11, 0, 0, 3, op)   # P12.L0.H0 is interpretable only for A3\n",
        "    add_one_pca_subplot(axs[1, 0], 13, 0, 0, 1, op)   # P13.L0.H0 is interpretable only for A1\n",
        "    add_one_pca_subplot(axs[1, 1], 14, 0, 0, 4, op)   # P14.L0.H0 is interpretable only for A4\n",
        "    add_one_pca_subplot(axs[2, 0], 14, 1, 1, 4, op)   # P14.L1.H1 is interpretable only for A4\n",
        "    add_one_pca_subplot(axs[2, 1], 15, 0, 0, 4, op)   # P15.L0.H0 is interpretable only for A4\n",
        "    add_one_pca_subplot(axs[3, 0], 15, 1, 1, 4, op)   # P15.L1.H1 is interpretable only for A4\n",
        "    add_one_pca_subplot(axs[3, 1], 15, 1, 2, 4, op)   # P15.L1.H2 is interpretable only for A4\n",
        "    add_one_pca_subplot(axs[4, 0], 16, 0, 0, 5, op)   # P16.L0.H0 is interpretable only for A5\n",
        "\n",
        "  lines_labels = [axs[0,0].get_legend_handles_labels()]\n",
        "  lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "  # fig.legend(lines, labels, loc='lower center', ncol=4)\n",
        "  # fig.subplots_adjust(bottom=0.2)  # Adjust the bottom spacing\n",
        "\n",
        "  if model_name == \"add_d5_l2_h3_train15K\" :\n",
        "    axs[3, 1].legend(lines, labels)\n",
        "    axs[3, 1].axis('off') # Now, to hide the last subplot\n",
        "  if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "    axs[4, 1].legend(lines, labels)\n",
        "    axs[4, 1].axis('off') # Now, to hide the last subplot\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file('PCA_Trigrams')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3611cpuEFhoW"
      },
      "source": [
        "#Part 19B: Results: add_d6_l2_h3_train15K PCA decomposition bi-state results\n",
        "\n",
        "Plot attention heads in the positions 8 to 16 with a clear \"bi-state\" response to (exactly) one An.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAT8Z54oMuur"
      },
      "outputs": [],
      "source": [
        "if not use_pca:\n",
        "  print( \"PCA library failed to import. So PCA not done\")\n",
        "\n",
        "if use_pca and model_name == \"add_d6_l2_h3_train15K\" :\n",
        "  op = PLUS_INDEX\n",
        "\n",
        "  fig, axs = plt.subplots(1, 2)\n",
        "  fig.set_figheight(2)\n",
        "  fig.set_figwidth(5)\n",
        "\n",
        "  if cfg.n_digits == 5 and cfg.n_layers == 2 and cfg.n_heads == 3:\n",
        "    # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "    add_one_pca_subplot(axs[0], 10, 0, 1, 0, op)   # P10.L0.H1 is clear only for A0\n",
        "    add_one_pca_subplot(axs[1], 15, 0, 1, 0, op)   # P15.L0.H1 is clear only for A0\n",
        "\n",
        "  if cfg.n_digits == 6 and cfg.n_layers == 2 and cfg.n_heads == 3:\n",
        "    # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "    add_one_pca_subplot(axs[0], 12, 0, 2, 0, op)   # P12.L0.H2 is clear only for A0\n",
        "\n",
        "\n",
        "  lines_labels = [axs[0].get_legend_handles_labels()]\n",
        "  lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "  fig.legend(lines, labels, loc='lower center', ncol=4)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file('PCA_Bigrams')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIu3Pr9CMx3l"
      },
      "source": [
        "#Part 19C: Results: PCA decomposition of useful cells\n",
        "\n",
        "Parts 19A and 19B are selective. This part is not. Use it to find (verify) the interesting parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdfpkXmAMzg4"
      },
      "outputs": [],
      "source": [
        "def graph_all_pca_results(op):\n",
        "  for useful_cell in ucfg.useful_cells:\n",
        "    if useful_cell.is_head():\n",
        "      position = useful_cell.position\n",
        "      layer = useful_cell.layer\n",
        "      head = useful_cell.head\n",
        "      print( \"PCA: position=\", position, \"layer=\", layer, \"head=\", head)\n",
        "\n",
        "      fig, axs = plt.subplots(4, 2)\n",
        "\n",
        "      add_one_pca_subplot(axs[0, 0], position, layer, head, 0, op)\n",
        "      add_one_pca_subplot(axs[0, 1], position, layer, head, 1, op)\n",
        "      add_one_pca_subplot(axs[1, 0], position, layer, head, 2, op)\n",
        "      add_one_pca_subplot(axs[1, 1], position, layer, head, 3, op)\n",
        "      add_one_pca_subplot(axs[2, 0], position, layer, head, 4, op)\n",
        "      add_one_pca_subplot(axs[2, 1], position, layer, head, 5, op)\n",
        "      add_one_pca_subplot(axs[3, 0], position, layer, head, 6, op)\n",
        "      add_one_pca_subplot(axs[3, 1], position, layer, head, 7, op)\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "if use_pca and model_name == \"add_d6_l2_h3_train15K\" :\n",
        "\n",
        "  if cfg.perc_add() > 0:\n",
        "    graph_all_pca_results(PLUS_INDEX)\n",
        "  if cfg.perc_sub > 0:\n",
        "    graph_all_pca_results(MINUS_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgCog0mYkYPV"
      },
      "source": [
        "# Part 21A : Set Up Interchange Interventions\n",
        "\n",
        "Here we test our mapping of our mathematical framework (causual abstraction) to the model attention heads.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8VoNc_ckfrJ"
      },
      "outputs": [],
      "source": [
        "class A_Config():\n",
        "  # A list of locations. Each location is a list with three values [token position, layer, head]\n",
        "  # An example list is [[11,0,0], [11,0,1]]\n",
        "  locations = [] # PQR TBDeveloped\n",
        "\n",
        "  # A list of stored weightings collected from the model.\n",
        "  # PQR TBDeveloped. Same length as locations\n",
        "  store = []\n",
        "\n",
        "\n",
        "  token_position : int  # The token position we want to get/set.\n",
        "  layer : int # The layer we want to get/set\n",
        "  heads = [] # The heads we want to get/set\n",
        "  threshold : int\n",
        "\n",
        "  questions = []\n",
        "  null_hooks = []\n",
        "  get_hooks = []\n",
        "  put_hooks = []\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.token_position = 10\n",
        "    self.layer = 0\n",
        "    self.heads = []\n",
        "    self.threshold = 0.00001\n",
        "    self.questions = []\n",
        "    self.store = []\n",
        "    self.null_hooks = []\n",
        "    self.get_hooks = []\n",
        "    self.put_hooks = []\n",
        "\n",
        "\n",
        "acfg = A_Config()\n",
        "acfg.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEal059xk_rI"
      },
      "outputs": [],
      "source": [
        "no_impact_str = \"(none)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMdSybNpnU0m"
      },
      "outputs": [],
      "source": [
        "# Get and put attention head value hooks\n",
        "\n",
        "def a_null_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  #print(\"In a_null_attn_z_hook\", value.shape)  # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "\n",
        "\n",
        "def a_get_l0_attn_z_hook(value, hook):\n",
        "  if acfg.layer == 0:\n",
        "    # print( \"In a_get_l0_attn_z_hook\", value.shape) # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "    acfg.store = value.clone()\n",
        "\n",
        "\n",
        "def a_get_l1_attn_z_hook(value, hook):\n",
        "  if acfg.layer == 1:\n",
        "    # print( \"In acfg.get_l1_attn_z_hook\", value.shape) # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "    acfg.store = value.clone()\n",
        "\n",
        "\n",
        "def a_put_l0_attn_z_hook(value, hook):\n",
        "  if acfg.layer == 0:\n",
        "    # print( \"In a_l0_attn_z_hook\", value.shape) # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "    for head_index in acfg.heads:\n",
        "      value[:,acfg.token_position,head_index,:] = acfg.store[:,acfg.token_position,head_index,:].clone()\n",
        "\n",
        "\n",
        "def a_put_l1_attn_z_hook(value, hook):\n",
        "  if acfg.layer == 1:\n",
        "    # print( \"In a_put_l1_attn_z_hook\", value.shape) # Get [1, 22, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "    for head_index in acfg.heads:\n",
        "      value[:,acfg.token_position,head_index,:] = acfg.store[:,acfg.token_position,head_index,:].clone()\n",
        "\n",
        "\n",
        "def a_reset(token_position, layer, heads):\n",
        "  global acfg\n",
        "\n",
        "  acfg.reset()\n",
        "\n",
        "  acfg.token_position = token_position\n",
        "  acfg.layer = layer\n",
        "  acfg.heads = heads\n",
        "\n",
        "  acfg.null_hooks = [(l_attn_hook_z_name[0], a_null_attn_z_hook)]\n",
        "  acfg.get_hooks = [(l_attn_hook_z_name[0], a_get_l0_attn_z_hook),(l_attn_hook_z_name[1], a_get_l1_attn_z_hook)]\n",
        "  acfg.put_hooks = [(l_attn_hook_z_name[0], a_put_l0_attn_z_hook),(l_attn_hook_z_name[1], a_put_l1_attn_z_hook)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SeIO06JnU7I"
      },
      "outputs": [],
      "source": [
        "def a_predict_question(description, the_hooks, always):\n",
        "  assert len(acfg.questions) == 1\n",
        "\n",
        "  main_model.reset_hooks()\n",
        "  main_model.set_use_attn_result(True)\n",
        "\n",
        "  all_logits = main_model.run_with_hooks(acfg.questions.cuda(), return_type=\"logits\", fwd_hooks=the_hooks)\n",
        "  all_losses_raw, all_max_prob_tokens = logits_to_tokens_loss(all_logits, acfg.questions.cuda())\n",
        "\n",
        "  loss_max = utils.to_numpy(loss_fn(all_losses_raw[0]).max())\n",
        "  answer_str = tokens_to_string(all_max_prob_tokens[0])\n",
        "\n",
        "  # Compare the question answer (ignoring the ablation intervention) to what the model generated (which is impacted by the ablation intervention)\n",
        "  impact_str = get_answer_impact( acfg.questions[0], answer_str )\n",
        "  if impact_str == \"\":\n",
        "    impact_str = no_impact_str\n",
        "\n",
        "  if always or (loss_max > acfg.threshold):\n",
        "    loss_str = no_impact_str if loss_max < 1e-7 else str(loss_max)\n",
        "\n",
        "    print(description, \"  ModelPredicts:\", answer_str, \"  DigitsImpacted:\", impact_str, \"  Loss:\", loss_str)\n",
        "\n",
        "  return answer_str, impact_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk0gVCF9Gr5D"
      },
      "outputs": [],
      "source": [
        "def a_run_intervention_core(token_position, layer, heads, store_question, alter_question, always = True):\n",
        "  a_reset(token_position, layer, heads)\n",
        "\n",
        "  # Predict first question and store activation values (including the Dn.BA)\n",
        "  acfg.questions = make_questions(PLUS_INDEX, [store_question])\n",
        "  a_predict_question(\"Unit test (null hook)\", acfg.null_hooks, False)\n",
        "  a_predict_question(\"Store activation\", acfg.get_hooks, False)\n",
        "\n",
        "  # Predict second question. Then rerun overriding PnLmHp to give bad answer\n",
        "  acfg.questions = make_questions(PLUS_INDEX, [alter_question])\n",
        "  a_predict_question(\"Unit test (null hook)\", acfg.null_hooks, False)\n",
        "  prompt = \"Intervening on P\" + str(token_position) + \"L\" + str(layer) + \"H\"\n",
        "  for head_index in acfg.heads:\n",
        "    prompt += str(head_index) + \",\"\n",
        "  return a_predict_question(prompt, acfg.put_hooks, always)\n",
        "\n",
        "\n",
        "def a_run_intervention(token_position, layer, heads, store_question, alter_question, expected_impact, expected_answer_int):\n",
        "\n",
        "    answer_str, impact_str = a_run_intervention_core(token_position, layer, heads, store_question, alter_question)\n",
        "\n",
        "    if expected_impact == \"\":\n",
        "      expected_impact = no_impact_str\n",
        "    expected_answer = int_to_str(expected_answer_int)\n",
        "\n",
        "    success = (impact_str == expected_impact) and (answer_str == expected_answer)\n",
        "    if not success:\n",
        "      print(\"Failed:\", impact_str, expected_impact, answer_str, expected_answer)\n",
        "\n",
        "    return success, answer_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bbeIfUxvLzl"
      },
      "source": [
        "# Part 21B : add_d6_l2_h3_train15K Interchange Interventions\n",
        "\n",
        "Here we test our mapping of our mathematical framework (casual abstraction) to the model attention heads.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iosx5zE_macF"
      },
      "source": [
        "## Part 21C: Dn.BA: Confirmed claim for add_d6_l2_h3_train15K, that P15..P20.H1&2 do BA calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-lFFjjgWAM_"
      },
      "outputs": [],
      "source": [
        "def int_to_str( n ):\n",
        "  s = str(abs(n))\n",
        "  while len(s) < cfg.n_digits + 1 :\n",
        "    s = \"0\" + s\n",
        "  s = (\"+\" if n >= 0 else \"-\") + s\n",
        "  return s\n",
        "\n",
        "if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "  assert int_to_str(1234) == \"+0001234\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Gs5yglncax"
      },
      "outputs": [],
      "source": [
        "def test_BA(position, layer, heads, alter_digit, expected_impact):\n",
        "  description= \"Test claim P\" + str(position) + \"L\" + str(layer)+ \"Head(s) perform D\"+str(alter_digit)+\".BA = (D\"+str(alter_digit)+\" + D\"+str(alter_digit)+\"') % 10 impacting \"+expected_impact+\" accuracy\"\n",
        "  print(description)\n",
        "\n",
        "  store_question = [222222, 333333] # Sum is 555555\n",
        "  alter_question = [555555, 444444] # Sum is 999999\n",
        "  expected_answer1 = 999999 + (5 - 9) * (10 ** alter_digit)\n",
        "  success1, _ = a_run_intervention(position, layer, heads, store_question, alter_question, expected_impact, expected_answer1)\n",
        "\n",
        "  store_question = [222222, 111111] # Sum is 333333\n",
        "  alter_question = [555555, 444444] # Sum is 999999\n",
        "  expected_answer2 = 999999 + (3 - 9) * (10 ** alter_digit)\n",
        "  success2, _ = a_run_intervention(position, layer, heads, store_question, alter_question, expected_impact, expected_answer2)\n",
        "\n",
        "  print( \"Claim confirmed\" if success1 and success2 else \"Claim failed\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gM8TpWeyysU"
      },
      "outputs": [],
      "source": [
        "if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "  test_BA( 20, 0, [1,2], 0, \"A0\")\n",
        "  test_BA( 19, 0, [1,2], 1, \"A1\")\n",
        "  test_BA( 18, 0, [1,2], 2, \"A2\")\n",
        "  test_BA( 17, 0, [1,2], 3, \"A3\")\n",
        "  test_BA( 16, 0, [1,2], 4, \"A4\")\n",
        "  test_BA( 15, 0, [1,2], 5, \"A5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_8z_UwsS_Xi"
      },
      "outputs": [],
      "source": [
        "def test_not_BA(position, layer, heads):\n",
        "  store_question = [222222, 111111] # Sum is 333333. No Dn.MC\n",
        "  alter_question = [333333, 555555] # Sum is 888888. No Dn.MC\n",
        "  expected_answer = alter_question[0] + alter_question[1]\n",
        "\n",
        "  return a_run_intervention(position, layer, heads, store_question, alter_question, no_impact_str, expected_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5DMV3I_25ST"
      },
      "source": [
        "## Part 21D: Dn.MC: Confirmed claim for add_d6_l2_h3_train15K, that P15..P19.H0 do MC calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4ZDlFk9RVNt"
      },
      "outputs": [],
      "source": [
        "def test_MC_core(position, layer, heads, alter_digit, expected_impact):\n",
        "\n",
        "  store_question = [222222, 777777] # Sum is 999999. No Dn.MC\n",
        "  alter_question = [333333, 555555] # Sum is 888888. No Dn.MC\n",
        "  alter_answer = alter_question[0] + alter_question[1]\n",
        "\n",
        "  diff = (7 - 5) * (10 ** alter_digit)\n",
        "  store_question[1] += diff # Now has Dn.MC in position alter_digit\n",
        "  expected_answer1 = alter_answer + (10 ** (alter_digit+1))\n",
        "\n",
        "  return a_run_intervention(position, layer, heads, store_question, alter_question, expected_impact, expected_answer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJBYT0hF3R_u"
      },
      "outputs": [],
      "source": [
        "def test_MC(position, layer, heads, alter_digit, expected_impact):\n",
        "  description= \"Test claim P\" + str(position) + \"L\" + str(layer)+ \"Head(s) perform D\"+str(alter_digit)+\".MC impacting \"+expected_impact+\" accuracy\"\n",
        "  print(description)\n",
        "\n",
        "  success1 = test_MC_core(position, layer, heads, alter_digit, expected_impact)\n",
        "\n",
        "  success2 = test_not_BA(position, layer, heads)\n",
        "\n",
        "  print( \"Claim confirmed\" if success1 and success2 else \"Claim failed\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5FZLpSe5VbP"
      },
      "outputs": [],
      "source": [
        "if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "  test_MC( 19, 0, [0], 0, \"A1\")\n",
        "  test_MC( 18, 0, [0], 1, \"A2\")\n",
        "  test_MC( 17, 0, [0], 2, \"A3\")\n",
        "  test_MC( 16, 0, [0], 3, \"A4\")\n",
        "  test_MC( 15, 0, [0], 4, \"A5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThzFD1FxBXg8"
      },
      "source": [
        "## Part 21E: Dn.C: Confirm Dn.C calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmvhYDj8XTnm"
      },
      "outputs": [],
      "source": [
        "def test_C_core(position, layer, heads, alter_digit, expected_impact):\n",
        "\n",
        "  store_question = [222222, 777777] # Sum is 999999. No Dn.MC\n",
        "  alter_question = [333333, 666666] # Sum is 999999. No Dn.MC\n",
        "  store_question[1] += (7 - 6) * (10 ** alter_digit) # Now has Dn.MC in position alter_digit\n",
        "\n",
        "  alter_question_str = int_to_str(alter_question[0] + alter_question[1])\n",
        "  expected_answer_str = int_to_str(alter_question[0] + alter_question[1] + (10 ** (alter_digit+1))) # alter_question with ablated value from store_question\n",
        "\n",
        "  model_answer_str, _ = a_run_intervention_core(position, layer, heads, store_question, alter_question, False)\n",
        "\n",
        "  local_impact_str = get_answer_impact_str(model_answer_str, alter_question_str)\n",
        "  if local_impact_str == \"\":\n",
        "    local_impact_str = no_impact_str\n",
        "\n",
        "  success = (local_impact_str == expected_impact) and (model_answer_str == expected_answer_str)\n",
        "  if not success:\n",
        "    print(\"Failed:\", local_impact_str, expected_impact, model_answer_str, expected_answer_str)\n",
        "  return success, local_impact_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-bcEooQBp2v"
      },
      "outputs": [],
      "source": [
        "def test_C(position, layer, heads, alter_digit, expected_impact):\n",
        "  description= \"Test claim P\" + str(position) + \"L\" + str(layer)+ \"Head(s) perform D\"+str(alter_digit)+\".C = TriCase(D\"+str(alter_digit)+\" + D\"+str(alter_digit)+\"') \"+expected_impact+\" accuracy\"\n",
        "  print(description)\n",
        "\n",
        "  success1 = test_not_BA(position, layer, heads)\n",
        "\n",
        "  success2, _ = test_C_core(position, layer, heads, alter_digit, expected_impact)\n",
        "\n",
        "  print( \"Claim confirmed\" if success1 and success2 else \"Claim failed\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBKdA7P2Tsjl"
      },
      "outputs": [],
      "source": [
        "if model_name == \"add_d6_l2_h3_train15K\" :\n",
        "  test_C( 13, 0, [1], 0, \"A654321\")\n",
        "  test_C( 11, 0, [1], 1, \"A65432\")\n",
        "  test_C( 11, 0, [0], 2, \"A6543\")\n",
        "\n",
        "  # Random search\n",
        "  # for position in range(11,14):\n",
        "  #   for head in range(2):\n",
        "  #     test_C( position, 0, [head], 3, \"A654\")\n",
        "\n",
        "  #test_C( 11, 0, [0], 3, \"A654\")\n",
        "  #test_C( 12, 0, [0], 3, \"A654\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMnvjPxN6wZ7"
      },
      "source": [
        "## Part 21F: TBA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFLdZkmEHhIo"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P8.L0.H1 performs V2.C = TriCase(D2, D2’) impacting A4 and A5 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [44444, 55555] # Sum is 099999. V2 has no MC.\n",
        "alter_question = [11111, 11111] # Sum is 022222. V2 has no MC.\n",
        "a_run_intervention(8, 0, [1], store_question, alter_question, no_impact_str, 22222)\n",
        "\n",
        "store_question = [77711, 22711] # Sum is 100422. V2 has MC\n",
        "alter_question = [44444, 55555] # Sum is 099999. V2 has no MC\n",
        "a_run_intervention( 8, 0, [1], store_question, alter_question, \"A54\", 109999)\n",
        "\n",
        "store_question = [17711, 22711] # Sum is 035422. V2 has MC\n",
        "alter_question = [ 4444,  5555] # Sum is 009999. V2 has no MC\n",
        "a_run_intervention(8, 0, [1], store_question, alter_question, \"A4\", 19999 )\n",
        "\n",
        "# Deprecated: Confirmed that P8.L0.H1 is: Based on D2 and D2'. Triggers on a V2 carry value. Provides \"carry 1\" used in A5 and A4 calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNtG_mlXeZKd"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P9.L0.H1 performs V1.C = TriCase(D1, D1’) impacting A5, A4 & A3 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [ 44444, 55555] # Sum is 099999. V1 has no MC.\n",
        "alter_question = [ 11111, 11111] # Sum is 022222. V1 has no MC\n",
        "a_run_intervention( 9, 0, [1], store_question, alter_question, no_impact_str, 22222 )\n",
        "\n",
        "store_question = [ 11171, 11171] # Sum is 022342. V1 has MC\n",
        "alter_question = [ 44444, 55555] # Sum is 099999. V1 has no MC.\n",
        "a_run_intervention( 9, 0, [1], store_question, alter_question, \"A543\", 100999 )\n",
        "\n",
        "store_question = [ 11171, 11171] # Sum is 022342. V1 has MC\n",
        "alter_question = [  4444,  5555] # Sum is 009999. V1 has no MC\n",
        "a_run_intervention( 9, 0, [1], store_question, alter_question, \"A43\", 10999 )\n",
        "\n",
        "store_question = [ 11171, 11171] # Sum is 022342. V1 has MC\n",
        "alter_question = [   444,   555] # Sum is 000999. V1 has no MC\n",
        "a_run_intervention( 9, 0, [1], store_question, alter_question, \"A3\", 1999 )\n",
        "\n",
        "# Deprecated: Confirmed that P9.L0.H1 is: Based on D1 and D1'. Triggers on a V1 carry value. Provides \"carry 1\" used in A5, A4 & A3 calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDt2LmNwiMGA"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P10.L0.H1 performs V1.C2 = TriAdd(V1.C, TriCase(D0, D0’)) impacting A5, A4, A3 & A2 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [ 11111, 33333] # Sum is 044444. V0 has no MC.\n",
        "alter_question = [ 44444, 55555] # Sum is 099999. V0 has no MC\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, no_impact_str, 99999 )\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [ 44444, 55555] # Sum is 099999. V0 has no MC\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A5432\", 100099 )\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [  4444,  5555] # Sum is 009999. V0 has no MC\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A432\", 10099 )\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [   444,   555] # Sum is 000999. V0 has no MC\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A32\", 1099 )\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [    44,    55] # Sum is 000099. V0 has no MC\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A2\", 199 )\n",
        "\n",
        "# Deprecated: Confirmed that P10.L0.H1 is: Based on D0 and D0'. Triggers on a V0 carry value. Provides \"carry 1\" used in A5, A4, A3 & A2 calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxGCodzep7qV"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P11.L0.H1 performs V3.C4 = TriAdd(TriCase(D3, D3’),TriAdd(V2.C,V1.C2)) impacting A5 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [44444, 44444] # Sum is 088888. V3 sums to 8 (has no MC).\n",
        "alter_question = [11111, 11111] # Sum is 022222. V3 has no MC.\n",
        "a_run_intervention( 11, 0, [1], store_question, alter_question, no_impact_str, 22222 )\n",
        "\n",
        "store_question = [16111, 13111] # Sum is 032111. V3 sums to 9 (has no MC).\n",
        "alter_question = [44444, 55555] # Sum is 099999. V3 has no MC\n",
        "a_run_intervention( 11, 0, [1], store_question, alter_question, no_impact_str, 99999 )\n",
        "\n",
        "store_question = [16111, 16111] # Sum is 032111. V3 has MC\n",
        "alter_question = [44444, 55555] # Sum is 099999. V3 has no MC\n",
        "a_run_intervention( 11, 0, [1], store_question, alter_question, \"A5\", 199999 )\n",
        "\n",
        "# Deprecated: Confirmed that P11.L0.H1 is: Based on D3 and D3'. Triggers on a V3 carry value. Provides \"carry 1\" used in A5 calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo55vCvCoMq7"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P11.L0.H2 performs V4.C = TriCase(D4, D4’) impacting A5 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [44444, 55555] # Sum is 099999. V4 has no MC.\n",
        "alter_question = [11111, 11111] # Sum is 022222. V4 has no MC.\n",
        "a_run_intervention(11, 0, [2], store_question, alter_question, no_impact_str, 22222)\n",
        "\n",
        "store_question = [71111, 71111] # Sum is 100422. V4 has MC\n",
        "alter_question = [44444, 55555] # Sum is 099999. V4 has no MC\n",
        "a_run_intervention( 11, 0, [2], store_question, alter_question, \"A5\", 199999 )\n",
        "\n",
        "# Deprecated: Confirmed that P9.L0.H2 is: Based on D4 and D4'. Triggers on a V4 carry value. Provides \"carry 1\" used in A5 calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jSE5S9UQFpn"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P12.L0.H0 and H2 performs V4.BA = (D4 + D4’) % 10 impacting A4 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [72222, 71111] # Sum is 143333\n",
        "alter_question = [12342, 56573] # Sum is 068915\n",
        "a_run_intervention( 12, 0, [0,2], store_question, alter_question, \"A4\", 48915 )\n",
        "\n",
        "# Deprecated: Confirmed that P12.L0.H0+H2 is: Adds D4 and D4'. Impacts A4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6UpIJbfPG6r"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P13.L0.H0 and H2 performs V3.BA = (D3 + D3’) % 10 impacting A3 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [23222, 13111] # Sum is 36333\n",
        "alter_question = [12342, 56573] # Sum is 68915\n",
        "a_run_intervention( 13, 0, [0,2], store_question, alter_question, \"A3\", 66915 )\n",
        "\n",
        "# Deprecated: Confirmed that P13.L0.H0+H2 is: Adds D3 and D3'. Impacts A3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXiOPyZ1ONcg"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P14.L0.H0 and H2 performs V2.BA = (D2 + D2’) % 10 impacting A2 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [22322, 11311] # Sum is 33633. No V1.MC\n",
        "alter_question = [12342, 56573] # Sum is 68915. Has V1.MC\n",
        "a_run_intervention( 14, 0, [0,2], store_question, alter_question, \"A2\", 68715)\n",
        "\n",
        "store_question = [22322, 11311] # Sum is 33633. No V1.MC\n",
        "alter_question = [12133, 56133] # Sum is 68266. No V1.MC\n",
        "a_run_intervention( 14, 0, [0,2], store_question, alter_question, \"A2\", 68666)\n",
        "\n",
        "# Deprecated: Confirmed that P12.L0.H0 and H2 both impact A2, and together sum D2 and D2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAzB924mQkyN"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P14.L0.H1 calculates V1.C1 but also relies on P10.V1.C2, impacting A2 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [55555, 44454] # Sum is 100009. Has V1.MC\n",
        "alter_question = [22222, 33333] # Sum is 055555. No V1.MC\n",
        "a_run_intervention( 14, 0, [1], store_question, alter_question, \"A2\", 55655 ) # Get 055655. Correct\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A2\", 55655 ) # Get 055555. No impact.\n",
        "\n",
        "store_question = [55590, 44490] # Sum is 100080. Has V1.MC\n",
        "alter_question = [12345, 54321] # Sum is 066666. No V1.MC\n",
        "a_run_intervention( 14, 0, [1], store_question, alter_question, \"A2\", 66766 ) # Get 066766. Correct\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A2\", 66766 ) # Get 066666. No impact.\n",
        "\n",
        "store_question = [12345, 54321] # Sum is 066666. No V1.MC\n",
        "alter_question = [55590, 44490] # Sum is 100080. Has V1.MC\n",
        "a_run_intervention( 14, 0, [1], store_question, alter_question, \"A2\", 100980 ) # Get 100980. Correct\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A2\", 100980 ) # Get 100080. No impact.\n",
        "\n",
        "# Above shows:\n",
        "# - P14.L0.H1 behaviour is different from P10.L0.H1 behaviour\n",
        "# - P14.L0.H1 does not simply copy P10.L0.H1 (although this would be a valid way to get perfect accuracy in A2)\n",
        "print()\n",
        "\n",
        "store_question = [12345, 54321] # Sum is 066666. No V1.MC\n",
        "alter_question = [55555, 44445] # Sum is 100000. Has V0.MC, V1.MC, V1.C2\n",
        "a_run_intervention( 14, 0, [1], store_question, alter_question, \"A2\", 100900 ) # Get 100900. Correct\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A2\", 99900 ) # Get 099900. Correct\n",
        "\n",
        "store_question = [22222, 33333] # Sum is 055555. No V1.MC\n",
        "alter_question = [66663, 33337] # Sum is 100000. Has V0.MC, V1.MC, V1.C2\n",
        "a_run_intervention( 14, 0, [1], store_question, alter_question, \"A2\", 100900 ) # Get 100900. Correct\n",
        "a_run_intervention( 10, 0, [1], store_question, alter_question, \"A2\", 99900 ) # Get 099900. Correct\n",
        "\n",
        "# Above shows:\n",
        "# - P14.L0.H1 does rely on P10.L0.H1 for V1.C2 information when V1.C != V1.C2\n",
        "# - P14.L0.H1 calculates V1.C information itself from D1+D1'.\n",
        "\n",
        "# Deprecated: Overall confirmed: P14.L0.H1 calculates V1.C1 but also relies on P10.V1.C2 when V1.C != V1.C2. Impacts A2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVrX9QmOnVCM"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P19.L0.H0 performs D0.MC = (D0 + D0’) / 10 impacting A1 accuracy\")\n",
        "\n",
        "store_question = [22244, 11149] # Sum is 33393. Has D0.MC\n",
        "alter_question = [12342, 56513] # Sum is 68855. No D0.MC\n",
        "success1, _ = a_run_intervention(19, 0, [0], store_question, alter_question, \"A1\", 68865)\n",
        "\n",
        "# Now test counter-claim that an intervention where both questions do NOT generate a D0.MC has NO impact on A1\n",
        "store_question = [22242, 11141] # Sum is 33383. No D0.MC\n",
        "alter_question = [12342, 56523] # Sum is 68865. No D0.MC\n",
        "success2, _ = a_run_intervention(19, 0, [0], store_question, alter_question, no_impact_str, 68865)\n",
        "\n",
        "print( \"Claim confirmed\" if success1 and success2 else \"Claim failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTA5SdXWmvgD"
      },
      "outputs": [],
      "source": [
        "print( \"Test claim that P18.L0.H0 performs D0.MC = (D0 + D0’) / 10 impacting A1 accuracy\")\n",
        "\n",
        "store_question = [22244, 11149] # Sum is 33393. Has V0.MC\n",
        "alter_question = [12342, 56513] # Sum is 68855. No V0.MC\n",
        "success1, _ = a_run_intervention(19, 0, [0], store_question, alter_question, \"A1\", 68865)\n",
        "\n",
        "# Now test counter-claim that an intervention where both questions do NOT generate a D0.MC has NO impact on A1\n",
        "store_question = [22242, 11141] # Sum is 33383. No V0.MC\n",
        "alter_question = [12342, 56523] # Sum is 68865. No V0.MC\n",
        "success2 = a_run_intervention(19, 0, [0], store_question, alter_question, no_impact_str, 68865)\n",
        "\n",
        "if success1 and success2 :\n",
        "  print( \"Claim confirmed\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTs6Cu55jB7y"
      },
      "source": [
        "#Part 22: MLP Visualisation (incomplete, on-hold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YviVHnBRjDcs"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "# number of questions in batch that generated sample_cache\n",
        "num_questions = varied_questions.shape[0]\n",
        "\n",
        "\n",
        "def get_mlp_data(data_set_name):\n",
        "\n",
        "  data_set = sample_cache[data_set_name]\n",
        "  # print( data_set_name + \" shape\", data_set.shape) # 239, 22, 2040 = num_questions, cfg.n_ctx, cfg.d_mlp\n",
        "\n",
        "  raw_data = data_set[:,-3]\n",
        "  # print( \"raw_data shape\", raw_data.shape) # 239, 2040 = num_questions, cfg.d_mlp\n",
        "\n",
        "  answer = einops.rearrange(raw_data, \"(x y) d_mlp -> x y d_mlp\", x=num_questions).cpu().numpy()\n",
        "  # print( \"answer shape\", answer.shape) # 239, 1, 2040 = num_questions, ??, cfg.d_mlp\n",
        "\n",
        "  return answer\n",
        "\n",
        "\n",
        "l0_mlp_hook_pre_sq = get_mlp_data('blocks.0.mlp.hook_pre')\n",
        "l0_mlp_hook_post_sq = get_mlp_data('blocks.0.mlp.hook_post')\n",
        "l1_mlp_hook_pre_sq = get_mlp_data('blocks.1.mlp.hook_pre') if cfg.n_layers > 1 else l0_mlp_hook_pre_sq\n",
        "l1_mlp_hook_post_sq = get_mlp_data('blocks.1.mlp.hook_post') if cfg.n_layers > 1 else l0_mlp_hook_post_sq\n",
        "\n",
        "\n",
        "def plot_mlp_neuron_activation(pos: int):\n",
        "    clear_output()\n",
        "\n",
        "    l0_mlp_pre_data = l0_mlp_hook_pre_sq[:,:,pos]\n",
        "    l0_mlp_post_data = l0_mlp_hook_post_sq[:,:,pos]\n",
        "    l1_mlp_pre_data = l1_mlp_hook_pre_sq[:,:,pos]\n",
        "    l1_mlp_post_data = l1_mlp_hook_post_sq[:,:,pos]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "    plot = axs[0].imshow(l1_mlp_pre_data, cmap='magma', vmin=0, vmax=1)\n",
        "    cbar = plt.colorbar(plot, fraction=0.1)\n",
        "    cbar.set_label(r'l0_mlp_pre_data {}'.format(pos))\n",
        "    #axs[0].set_ylim(-0.5, 99.5)\n",
        "    #axs[0].set_yticks(range(100), labels=range(100), size=5.5);\n",
        "    #axs[0].set_xticks(range(100), labels=range(100), size=5.5, rotation='vertical');\n",
        "\n",
        "    plot = axs[1].imshow(l1_mlp_post_data, cmap='magma', vmin=0, vmax=1)\n",
        "    cbar = plt.colorbar(plot, fraction=0.1)\n",
        "    cbar.set_label(r'l0_mlp_post_data {}'.format(pos))\n",
        "    #axs[0].set_ylim(-0.5, 99.5)\n",
        "    #axs[0].set_yticks(range(100), labels=range(100), size=5.5);\n",
        "    #axs[0].set_xticks(range(100), labels=range(100), size=5.5, rotation='vertical');\n",
        "\n",
        "\n",
        "interact(plot_mlp_neuron_activation, pos=widgets.IntText(value=0, description='Index:'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PjaQvhhayUL"
      },
      "source": [
        "# Part 25 : Is the model 100% accurate?\n",
        "\n",
        "This is hard to prove. Can the model do 1M predictions without error?\n",
        "\n",
        "Takes ~25 mins to run (successfully) for ins_mix_d6_l3_h4_train40K_seed372001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlLOZMVkeqvm"
      },
      "outputs": [],
      "source": [
        "def null_hook(value, hook):\n",
        "  global verbose\n",
        "\n",
        "  verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMYZTv5HUp5F"
      },
      "outputs": [],
      "source": [
        "def one_million_questions_core():\n",
        "  global verbose\n",
        "  global ds\n",
        "\n",
        "  verbose = True\n",
        "\n",
        "  cfg.analysis_seed = 345621 # Randomly chosen\n",
        "  ds = data_generator() # Re-initialise the data generator\n",
        "\n",
        "  the_successes = 0\n",
        "  the_fails = 0\n",
        "\n",
        "  num_batches = 1000000//cfg.batch_size\n",
        "  for epoch in range(num_batches):\n",
        "      tokens = next(ds)\n",
        "\n",
        "      the_hook = [(l_attn_hook_z_name[0], null_hook)]\n",
        "      q_predict_questions(tokens, the_hook)\n",
        "\n",
        "      the_fails = q_total_complexity_fails()\n",
        "      if the_fails> 0:\n",
        "        break\n",
        "\n",
        "      the_successes = the_successes + cfg.batch_size\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "          print(\"Batch\", epoch, \"of\", num_batches, \"#Successes=\", the_successes)\n",
        "\n",
        "  print(\"successes\", the_successes, \"num_fails\", the_fails)\n",
        "  if the_fails > 0:\n",
        "    \"WARNING: Model is not fully accurate. It failed the 1M Q test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsauYqjaxok"
      },
      "outputs": [],
      "source": [
        "def one_million_questions():\n",
        "  print_config()\n",
        "  print()\n",
        "\n",
        "  if model_might_be_fully_accurate:\n",
        "\n",
        "    # Commented out as it takes > 9 minutes to run\n",
        "    if cfg.perc_add() > 0 and cfg.perc_sub > 0:\n",
        "      print(\"Subtraction:\")\n",
        "      cfg.perc_sub = 100\n",
        "      one_million_questions_core()\n",
        "      print()\n",
        "      print(\"Addition:\")\n",
        "      cfg.perc_sub = 0\n",
        "      one_million_questions_core()\n",
        "\n",
        "    else:\n",
        "      # Predict 1M (sub, add or mult) questions\n",
        "      one_million_questions_core()\n",
        "\n",
        "  else:\n",
        "    print(\"WARNING: Model is not fully accurate. It failed some test questions\")\n",
        "\n",
        "\n",
        "# Takes ~25 minutes to run\n",
        "# one_million_questions()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pTd3nmsMJV5T",
        "aQNjIosyX9Y-",
        "P8RfHXneJw6n",
        "ZHiJhch4KCej",
        "-KJhCxFtNKfm",
        "fGxoBWHNKRf0",
        "D6FwJW0tv4Nf",
        "Kw4jCYh-lTfW",
        "nXBYdxj-jLZc",
        "Iosx5zE_macF",
        "Z5DMV3I_25ST",
        "2PjaQvhhayUL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}